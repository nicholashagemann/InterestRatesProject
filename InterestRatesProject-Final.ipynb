{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89b1979",
   "metadata": {},
   "source": [
    "# Predicting Interest Rates for Bank Loans\n",
    "In this project, I use a dataset of loans from a bank to predict the interest rate an applicant can expect\n",
    "- first, I wrangle the data by cleansing it of missing values, useless variables, and leaky information\n",
    "- then, I feature engineer a few columns to hopefully improve my model's performance\n",
    "- finally, I iterate through various models, optimizing hyperparameters for each, to find the best model\n",
    "- I obtain a model which can account for 90% of the variance in the data and predict interest rate with a mean\n",
    "error of 0.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "419624b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, validation_curve # k-fold CV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03985d",
   "metadata": {},
   "source": [
    "# Data Wrangling and Feature Engineering\n",
    "Here, I clean my data into a format that will be a suitable input for modeling. I get rid of columns with high cardinality or a cardinality of 1, excessive NaN values, as well as all the columns containing leaky information. The leaky variables were variables in the dataset available after the loan had been approved - had I not gotten rid of them, my model would have had much greater predictive power (I tested this for myself but running the same analyses without a fully wrangled dataframe - that analysis is not included here) but it would have been a *useless* model since the majority of the predictive power would have come from columns like the 'grade' and 'subgrade' columns describing how likely the bank thought the applicant was to pay back the loan, which is very tightly correlated with the interest rate. The more risky a loan is for the bank, the higher interest they will charge on it.\n",
    "\n",
    "Although it is not obvious from the code, I found the documentation to this dataset and meticulously examined each column to determine if it should be kept, removed, or changed to a more suitable format. \n",
    "\n",
    "I engineer a few features, several of which proved to have strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0cdf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my dataframe is quite large, starting at 150 columns and over 100,000 rows. \n",
    "# I will do some cleaning and feature engineering, but first I need to see all my columns\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac51c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>url</th>\n",
       "      <th>desc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "      <th>annual_inc_joint</th>\n",
       "      <th>dti_joint</th>\n",
       "      <th>verification_status_joint</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>open_acc_6m</th>\n",
       "      <th>open_act_il</th>\n",
       "      <th>open_il_12m</th>\n",
       "      <th>open_il_24m</th>\n",
       "      <th>mths_since_rcnt_il</th>\n",
       "      <th>total_bal_il</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>...</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_bc_dlq</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>mths_since_recent_revol_delinq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>revol_bal_joint</th>\n",
       "      <th>sec_app_fico_range_low</th>\n",
       "      <th>sec_app_fico_range_high</th>\n",
       "      <th>sec_app_earliest_cr_line</th>\n",
       "      <th>sec_app_inq_last_6mths</th>\n",
       "      <th>sec_app_mort_acc</th>\n",
       "      <th>sec_app_open_acc</th>\n",
       "      <th>sec_app_revol_util</th>\n",
       "      <th>sec_app_open_act_il</th>\n",
       "      <th>sec_app_num_rev_accts</th>\n",
       "      <th>sec_app_chargeoff_within_12_mths</th>\n",
       "      <th>sec_app_collections_12_mths_ex_med</th>\n",
       "      <th>sec_app_mths_since_last_major_derog</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>hardship_type</th>\n",
       "      <th>hardship_reason</th>\n",
       "      <th>hardship_status</th>\n",
       "      <th>deferral_term</th>\n",
       "      <th>hardship_amount</th>\n",
       "      <th>hardship_start_date</th>\n",
       "      <th>hardship_end_date</th>\n",
       "      <th>payment_plan_start_date</th>\n",
       "      <th>hardship_length</th>\n",
       "      <th>hardship_dpd</th>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <th>orig_projected_additional_accrued_interest</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>109889419.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.44%</td>\n",
       "      <td>640.10</td>\n",
       "      <td>B</td>\n",
       "      <td>B1</td>\n",
       "      <td>IT Support</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>May-2017</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>296xx</td>\n",
       "      <td>SC</td>\n",
       "      <td>17.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb-2005</td>\n",
       "      <td>690.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12835.0</td>\n",
       "      <td>41%</td>\n",
       "      <td>28.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21657.670000</td>\n",
       "      <td>21657.67</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1657.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jun-2018</td>\n",
       "      <td>1002.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-2018</td>\n",
       "      <td>744.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>159516.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59354.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10634.0</td>\n",
       "      <td>15465.0</td>\n",
       "      <td>45.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206029.0</td>\n",
       "      <td>72189.0</td>\n",
       "      <td>28300.0</td>\n",
       "      <td>84078.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68579794.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.80%</td>\n",
       "      <td>694.95</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>Engineer Tech</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Jan-2016</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>219xx</td>\n",
       "      <td>MD</td>\n",
       "      <td>18.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jun-1995</td>\n",
       "      <td>680.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17498.0</td>\n",
       "      <td>88.4%</td>\n",
       "      <td>16.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24983.207947</td>\n",
       "      <td>24983.21</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>3383.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov-2018</td>\n",
       "      <td>8.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May-2019</td>\n",
       "      <td>709.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33987.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4746.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22405.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>91.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337728.0</td>\n",
       "      <td>51485.0</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>38591.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10105952.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>24950.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>17.57%</td>\n",
       "      <td>898.43</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>Retail manager</td>\n",
       "      <td>7 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>90100.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2013</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>Borrower added on 12/22/13 &gt; credit card and...</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>home</td>\n",
       "      <td>017xx</td>\n",
       "      <td>MA</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mar-1994</td>\n",
       "      <td>695.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12440.0</td>\n",
       "      <td>49%</td>\n",
       "      <td>36.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32343.306742</td>\n",
       "      <td>32278.62</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>7343.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dec-2016</td>\n",
       "      <td>898.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nov-2017</td>\n",
       "      <td>724.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250991.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16733.0</td>\n",
       "      <td>5275.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278585.0</td>\n",
       "      <td>59252.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>61185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>57713137.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.89%</td>\n",
       "      <td>154.14</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>3 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>110139.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Aug-2015</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Credit card refinancing</td>\n",
       "      <td>641xx</td>\n",
       "      <td>MO</td>\n",
       "      <td>33.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Oct-1998</td>\n",
       "      <td>715.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29415.0</td>\n",
       "      <td>84.8%</td>\n",
       "      <td>52.0</td>\n",
       "      <td>w</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5492.542327</td>\n",
       "      <td>5492.54</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>492.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sep-2017</td>\n",
       "      <td>1495.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May-2019</td>\n",
       "      <td>559.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333944.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>364655.0</td>\n",
       "      <td>152992.0</td>\n",
       "      <td>13900.0</td>\n",
       "      <td>138487.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11966074.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11850.0</td>\n",
       "      <td>11850.0</td>\n",
       "      <td>11850.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.16%</td>\n",
       "      <td>405.93</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>Account Supervisor</td>\n",
       "      <td>6 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>50500.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Feb-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>https://lendingclub.com/browse/loanDetail.acti...</td>\n",
       "      <td>Borrower added on 02/18/14 &gt; I'm trying to g...</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Credit card refinancing</td>\n",
       "      <td>373xx</td>\n",
       "      <td>TN</td>\n",
       "      <td>20.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Apr-2003</td>\n",
       "      <td>670.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6774.0</td>\n",
       "      <td>61.6%</td>\n",
       "      <td>15.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13560.270000</td>\n",
       "      <td>13560.27</td>\n",
       "      <td>11850.0</td>\n",
       "      <td>1710.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Apr-2015</td>\n",
       "      <td>8283.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan-2019</td>\n",
       "      <td>679.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>47748.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5305.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>86.7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59771.0</td>\n",
       "      <td>47748.0</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>48771.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           id  member_id  loan_amnt  funded_amnt  \\\n",
       "0           0  109889419.0        NaN    20000.0      20000.0   \n",
       "1           1   68579794.0        NaN    21600.0      21600.0   \n",
       "2           2   10105952.0        NaN    25000.0      25000.0   \n",
       "3           3   57713137.0        NaN     5000.0       5000.0   \n",
       "4           4   11966074.0        NaN    11850.0      11850.0   \n",
       "\n",
       "   funded_amnt_inv        term int_rate  installment grade sub_grade  \\\n",
       "0          20000.0   36 months    9.44%       640.10     B        B1   \n",
       "1          21600.0   36 months    9.80%       694.95     B        B3   \n",
       "2          24950.0   36 months   17.57%       898.43     D        D2   \n",
       "3           5000.0   36 months    6.89%       154.14     A        A3   \n",
       "4          11850.0   36 months   14.16%       405.93     C        C2   \n",
       "\n",
       "            emp_title emp_length home_ownership  annual_inc  \\\n",
       "0          IT Support  10+ years       MORTGAGE     99500.0   \n",
       "1       Engineer Tech  10+ years       MORTGAGE     79000.0   \n",
       "2      Retail manager    7 years       MORTGAGE     90100.0   \n",
       "3    Business Analyst    3 years       MORTGAGE    110139.0   \n",
       "4  Account Supervisor    6 years           RENT     50500.0   \n",
       "\n",
       "  verification_status   issue_d loan_status pymnt_plan  \\\n",
       "0     Source Verified  May-2017  Fully Paid          n   \n",
       "1            Verified  Jan-2016  Fully Paid          n   \n",
       "2     Source Verified  Dec-2013  Fully Paid          n   \n",
       "3            Verified  Aug-2015  Fully Paid          n   \n",
       "4     Source Verified  Feb-2014  Fully Paid          n   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "1  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "2  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "3  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "4  https://lendingclub.com/browse/loanDetail.acti...   \n",
       "\n",
       "                                                desc             purpose  \\\n",
       "0                                                NaN  debt_consolidation   \n",
       "1                                                NaN  debt_consolidation   \n",
       "2    Borrower added on 12/22/13 > credit card and...    home_improvement   \n",
       "3                                                NaN         credit_card   \n",
       "4    Borrower added on 02/18/14 > I'm trying to g...         credit_card   \n",
       "\n",
       "                     title zip_code addr_state    dti  delinq_2yrs  \\\n",
       "0       Debt consolidation    296xx         SC  17.19          0.0   \n",
       "1       Debt consolidation    219xx         MD  18.94          1.0   \n",
       "2                     home    017xx         MA  19.58          1.0   \n",
       "3  Credit card refinancing    641xx         MO  33.01          0.0   \n",
       "4  Credit card refinancing    373xx         TN  20.82          0.0   \n",
       "\n",
       "  earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  \\\n",
       "0         Feb-2005           690.0            694.0             0.0   \n",
       "1         Jun-1995           680.0            684.0             0.0   \n",
       "2         Mar-1994           695.0            699.0             0.0   \n",
       "3         Oct-1998           715.0            719.0             0.0   \n",
       "4         Apr-2003           670.0            674.0             2.0   \n",
       "\n",
       "   mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  \\\n",
       "0                     NaN                     NaN      15.0      0.0   \n",
       "1                    12.0                     NaN      13.0      0.0   \n",
       "2                     0.0                     NaN      15.0      0.0   \n",
       "3                     NaN                     NaN      19.0      0.0   \n",
       "4                    69.0                     NaN       9.0      0.0   \n",
       "\n",
       "   revol_bal revol_util  total_acc initial_list_status  out_prncp  \\\n",
       "0    12835.0        41%       28.0                   w        0.0   \n",
       "1    17498.0      88.4%       16.0                   w        0.0   \n",
       "2    12440.0        49%       36.0                   f        0.0   \n",
       "3    29415.0      84.8%       52.0                   w        0.0   \n",
       "4     6774.0      61.6%       15.0                   f        0.0   \n",
       "\n",
       "   out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
       "0            0.0  21657.670000         21657.67          20000.0   \n",
       "1            0.0  24983.207947         24983.21          21600.0   \n",
       "2            0.0  32343.306742         32278.62          25000.0   \n",
       "3            0.0   5492.542327          5492.54           5000.0   \n",
       "4            0.0  13560.270000         13560.27          11850.0   \n",
       "\n",
       "   total_rec_int  total_rec_late_fee  recoveries  collection_recovery_fee  \\\n",
       "0        1657.67                 0.0         0.0                      0.0   \n",
       "1        3383.21                 0.0         0.0                      0.0   \n",
       "2        7343.31                 0.0         0.0                      0.0   \n",
       "3         492.54                 0.0         0.0                      0.0   \n",
       "4        1710.27                 0.0         0.0                      0.0   \n",
       "\n",
       "  last_pymnt_d  last_pymnt_amnt  next_pymnt_d last_credit_pull_d  \\\n",
       "0     Jun-2018          1002.69           NaN           Jul-2018   \n",
       "1     Nov-2018             8.94           NaN           May-2019   \n",
       "2     Dec-2016           898.26           NaN           Nov-2017   \n",
       "3     Sep-2017          1495.09           NaN           May-2019   \n",
       "4     Apr-2015          8283.18           NaN           Jan-2019   \n",
       "\n",
       "   last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
       "0                 744.0                740.0                         0.0   \n",
       "1                 709.0                705.0                         0.0   \n",
       "2                 724.0                720.0                         0.0   \n",
       "3                 559.0                555.0                         0.0   \n",
       "4                 679.0                675.0                         0.0   \n",
       "\n",
       "   mths_since_last_major_derog  policy_code application_type  \\\n",
       "0                          NaN          1.0       Individual   \n",
       "1                          NaN          1.0       Individual   \n",
       "2                         31.0          1.0       Individual   \n",
       "3                          NaN          1.0       Individual   \n",
       "4                         69.0          1.0       Individual   \n",
       "\n",
       "   annual_inc_joint  dti_joint verification_status_joint  acc_now_delinq  \\\n",
       "0               NaN        NaN                       NaN             0.0   \n",
       "1               NaN        NaN                       NaN             0.0   \n",
       "2               NaN        NaN                       NaN             1.0   \n",
       "3               NaN        NaN                       NaN             0.0   \n",
       "4               NaN        NaN                       NaN             0.0   \n",
       "\n",
       "   tot_coll_amt  tot_cur_bal  open_acc_6m  open_act_il  open_il_12m  \\\n",
       "0        1185.0     159516.0          2.0          3.0          0.0   \n",
       "1           0.0     291267.0          0.0          2.0          2.0   \n",
       "2           0.0     250991.0          NaN          NaN          NaN   \n",
       "3           0.0     333944.0          NaN          NaN          NaN   \n",
       "4          85.0      47748.0          NaN          NaN          NaN   \n",
       "\n",
       "   open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  \\\n",
       "0          2.0                16.0       59354.0     70.0          2.0   \n",
       "1          2.0                 7.0       33987.0     88.0          0.0   \n",
       "2          NaN                 NaN           NaN      NaN          NaN   \n",
       "3          NaN                 NaN           NaN      NaN          NaN   \n",
       "4          NaN                 NaN           NaN      NaN          NaN   \n",
       "\n",
       "   open_rv_24m  max_bal_bc  all_util  ...  inq_fi  total_cu_tl  inq_last_12m  \\\n",
       "0          2.0      4810.0      57.0  ...     2.0          0.0           1.0   \n",
       "1          0.0      4746.0      88.0  ...     0.0          0.0           0.0   \n",
       "2          NaN         NaN       NaN  ...     NaN          NaN           NaN   \n",
       "3          NaN         NaN       NaN  ...     NaN          NaN           NaN   \n",
       "4          NaN         NaN       NaN  ...     NaN          NaN           NaN   \n",
       "\n",
       "   acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0                   4.0      10634.0         15465.0     45.4   \n",
       "1                   2.0      22405.0           897.0     91.1   \n",
       "2                   3.0      16733.0          5275.0     62.3   \n",
       "3                   4.0      17576.0           282.0     98.0   \n",
       "4                   6.0       5305.0          2072.0     55.9   \n",
       "\n",
       "   chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  \\\n",
       "0                       0.0          0.0               147.0   \n",
       "1                       0.0          0.0               101.0   \n",
       "2                       0.0          0.0               178.0   \n",
       "3                       0.0          0.0               202.0   \n",
       "4                       0.0          0.0               130.0   \n",
       "\n",
       "   mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  \\\n",
       "0                 115.0                    3.0             3.0       4.0   \n",
       "1                 247.0                   27.0             7.0       4.0   \n",
       "2                 237.0                   21.0            11.0       3.0   \n",
       "3                 200.0                    4.0             4.0       2.0   \n",
       "4                 105.0                    2.0             2.0       0.0   \n",
       "\n",
       "   mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  \\\n",
       "0                   3.0                       NaN                    7.0   \n",
       "1                  27.0                      12.0                    NaN   \n",
       "2                  25.0                       0.0                    8.0   \n",
       "3                  68.0                       NaN                    4.0   \n",
       "4                   6.0                       NaN                    2.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq  num_accts_ever_120_pd  num_actv_bc_tl  \\\n",
       "0                             NaN                    0.0             4.0   \n",
       "1                            12.0                    0.0             3.0   \n",
       "2                             0.0                    2.0             3.0   \n",
       "3                             NaN                    0.0             1.0   \n",
       "4                             NaN                    2.0             2.0   \n",
       "\n",
       "   num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  \\\n",
       "0              4.0          7.0        9.0        8.0           11.0   \n",
       "1              7.0          4.0        4.0        4.0            8.0   \n",
       "2              5.0          4.0        8.0       18.0            9.0   \n",
       "3              4.0          1.0        9.0       33.0            4.0   \n",
       "4              5.0          2.0        4.0        7.0            5.0   \n",
       "\n",
       "   num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  \\\n",
       "0           16.0                  4.0      15.0               0.0   \n",
       "1            8.0                  7.0      13.0               0.0   \n",
       "2           15.0                  5.0      15.0               0.0   \n",
       "3           17.0                  4.0      19.0               0.0   \n",
       "4            8.0                  5.0       9.0               0.0   \n",
       "\n",
       "   num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "0           0.0                 0.0                 2.0           100.0   \n",
       "1           0.0                 0.0                 2.0            93.7   \n",
       "2           1.0                 0.0                 1.0            86.1   \n",
       "3           0.0                 0.0                 3.0           100.0   \n",
       "4           0.0                 0.0                 2.0            86.7   \n",
       "\n",
       "   percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  \\\n",
       "0              28.6                   0.0        0.0         206029.0   \n",
       "1              75.0                   0.0        0.0         337728.0   \n",
       "2              25.0                   0.0        0.0         278585.0   \n",
       "3             100.0                   0.0        0.0         364655.0   \n",
       "4              50.0                   0.0        0.0          59771.0   \n",
       "\n",
       "   total_bal_ex_mort  total_bc_limit  total_il_high_credit_limit  \\\n",
       "0            72189.0         28300.0                     84078.0   \n",
       "1            51485.0         10100.0                     38591.0   \n",
       "2            59252.0         14000.0                     61185.0   \n",
       "3           152992.0         13900.0                    138487.0   \n",
       "4            47748.0          4700.0                     48771.0   \n",
       "\n",
       "   revol_bal_joint  sec_app_fico_range_low  sec_app_fico_range_high  \\\n",
       "0              NaN                     NaN                      NaN   \n",
       "1              NaN                     NaN                      NaN   \n",
       "2              NaN                     NaN                      NaN   \n",
       "3              NaN                     NaN                      NaN   \n",
       "4              NaN                     NaN                      NaN   \n",
       "\n",
       "   sec_app_earliest_cr_line sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
       "0                       NaN                    NaN               NaN   \n",
       "1                       NaN                    NaN               NaN   \n",
       "2                       NaN                    NaN               NaN   \n",
       "3                       NaN                    NaN               NaN   \n",
       "4                       NaN                    NaN               NaN   \n",
       "\n",
       "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  \\\n",
       "0               NaN                 NaN                  NaN   \n",
       "1               NaN                 NaN                  NaN   \n",
       "2               NaN                 NaN                  NaN   \n",
       "3               NaN                 NaN                  NaN   \n",
       "4               NaN                 NaN                  NaN   \n",
       "\n",
       "   sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  \\\n",
       "0                    NaN                               NaN   \n",
       "1                    NaN                               NaN   \n",
       "2                    NaN                               NaN   \n",
       "3                    NaN                               NaN   \n",
       "4                    NaN                               NaN   \n",
       "\n",
       "   sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog  \\\n",
       "0                                 NaN                                  NaN   \n",
       "1                                 NaN                                  NaN   \n",
       "2                                 NaN                                  NaN   \n",
       "3                                 NaN                                  NaN   \n",
       "4                                 NaN                                  NaN   \n",
       "\n",
       "   hardship_flag hardship_type hardship_reason hardship_status deferral_term  \\\n",
       "0              N           NaN             NaN             NaN           NaN   \n",
       "1              N           NaN             NaN             NaN           NaN   \n",
       "2              N           NaN             NaN             NaN           NaN   \n",
       "3              N           NaN             NaN             NaN           NaN   \n",
       "4              N           NaN             NaN             NaN           NaN   \n",
       "\n",
       "   hardship_amount  hardship_start_date hardship_end_date  \\\n",
       "0              NaN                  NaN               NaN   \n",
       "1              NaN                  NaN               NaN   \n",
       "2              NaN                  NaN               NaN   \n",
       "3              NaN                  NaN               NaN   \n",
       "4              NaN                  NaN               NaN   \n",
       "\n",
       "  payment_plan_start_date hardship_length  hardship_dpd  hardship_loan_status  \\\n",
       "0                     NaN             NaN           NaN                   NaN   \n",
       "1                     NaN             NaN           NaN                   NaN   \n",
       "2                     NaN             NaN           NaN                   NaN   \n",
       "3                     NaN             NaN           NaN                   NaN   \n",
       "4                     NaN             NaN           NaN                   NaN   \n",
       "\n",
       "  orig_projected_additional_accrued_interest  hardship_payoff_balance_amount  \\\n",
       "0                                        NaN                             NaN   \n",
       "1                                        NaN                             NaN   \n",
       "2                                        NaN                             NaN   \n",
       "3                                        NaN                             NaN   \n",
       "4                                        NaN                             NaN   \n",
       "\n",
       "   hardship_last_payment_amount  debt_settlement_flag  \\\n",
       "0                           NaN                     N   \n",
       "1                           NaN                     N   \n",
       "2                           NaN                     N   \n",
       "3                           NaN                     N   \n",
       "4                           NaN                     N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "  settlement_amount  settlement_percentage  settlement_term  \n",
       "0               NaN                    NaN              NaN  \n",
       "1               NaN                    NaN              NaN  \n",
       "2               NaN                    NaN              NaN  \n",
       "3               NaN                    NaN              NaN  \n",
       "4               NaN                    NaN              NaN  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/nicholashagemann/Lambda/Datasets/loan_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36e44f",
   "metadata": {},
   "source": [
    "Note: The following code block is the final result of much time and computing power spent analyzing the dataframe - I have removed the code it took to create the wrangle function as its length would only serve to clutter my notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368afc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in the data is a specific loan that was issued by a bank\n",
    "# I choose the issue date of the loan to be the index\n",
    "df = pd.read_csv('/Users/nicholashagemann/Lambda/Datasets/loan_data.csv', \n",
    "                 parse_dates = ['issue_d', 'earliest_cr_line'], \n",
    "                 index_col = 'issue_d').sort_values('issue_d')\n",
    "\n",
    "# we will need to wrangle this data quite a bit before it's suitable for predicting interest rate\n",
    "def wrangle(df):\n",
    "    # Begin Data Cleaning\n",
    "    \n",
    "    # first I get rid of all columns with more than 20% NaN values\n",
    "    # note that the necessary columns will be dropped at the same time at the end of wrangle()\n",
    "    nan_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum()/len(df) > 0.2:\n",
    "            nan_cols.append(col)\n",
    "\n",
    "    # each of the following columns had data that was only available after the loan had been approved\n",
    "    # we want to predict the interest rate for a loan before the bank decides to approve the loan\n",
    "    # thus, these columns contain leakage and should be dropped\n",
    "    leaky_cols = ['out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'sub_grade',\n",
    "                  'total_rec_int', 'total_rec_prncp', 'total_rec_late_fee', 'recoveries',\n",
    "                  'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'initial_list_status',\n",
    "                  'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med',\n",
    "                  'funded_amnt', 'funded_amnt_inv', 'debt_settlement_flag', 'grade', 'loan_status']\n",
    "\n",
    "    # the following columns did not contain any useful data or were duplicates of other columns\n",
    "    cols_to_drop = ['title', 'zip_code', 'addr_state', 'fico_range_low',\n",
    "                    'Unnamed: 0', 'emp_title', 'url', 'id', 'last_credit_pull_d',\n",
    "                    'earliest_cr_line']\n",
    "    \n",
    "    # if a column only has one value it has no predictive power\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    # Begin Feature Engineering\n",
    "\n",
    "    # first I create a feature which identifies how old the borrower's credit is in days\n",
    "    df['credit_age'] = df.index - df['earliest_cr_line']\n",
    "    df['credit_age'].astype('timedelta64[D]').astype(int)\n",
    "    df['credit_age'] = df['credit_age'].dt.days.astype(int)\n",
    "\n",
    "    # we want our interest rate to be a float, not a string\n",
    "    # do the same for the amount of credit the borrower is using\n",
    "    df['int_rate'] = df['int_rate'].str.replace('%', '').astype(float)\n",
    "    df['revol_util'] = df['revol_util'].str.replace('%', '').astype(float)\n",
    "\n",
    "    # we want to represent length of the loan as an integer\n",
    "    df.loc[df['term'] == ' 36 months', 'term'] = 36\n",
    "    df.loc[df['term'] == ' 60 months', 'term'] = 60\n",
    "    df['term'] = df['term'].astype(int)\n",
    "\n",
    "    # we want to represent employment length as an integer\n",
    "    df.loc[df['emp_length'] == '< 1 year', 'emp_length'] = 0\n",
    "    df['emp_length'] = df['emp_length'].str.replace(' ', '').str.replace('year', '').str.replace('s', '').str.replace('+', '')\n",
    "    df['emp_length'] = pd.to_numeric(df['emp_length'], downcast = 'integer', errors = 'ignore')\n",
    "\n",
    "    # we want to represent whether the application was individual or joint as binary\n",
    "    df.loc[df['application_type'] == 'Individual', 'application_type'] = 1\n",
    "    df.loc[df['application_type'] == 'Joint App', 'application_type'] = 0\n",
    "    df['application_type'] = df['application_type'].astype(int)\n",
    "\n",
    "    # drop columns from the data cleaning\n",
    "    df.drop(columns = nan_cols + leaky_cols + cols_to_drop, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = wrangle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12728970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home_ownership          6\n",
       "verification_status     3\n",
       "purpose                14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many unique values we have for each of our object columns\n",
    "# looks like it's small enough to use OneHotEncoder\n",
    "df.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407f844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is:  (128334, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_d</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-07-01</th>\n",
       "      <td>3500.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.28</td>\n",
       "      <td>113.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENT</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>moving</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1882.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-01</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.43</td>\n",
       "      <td>155.38</td>\n",
       "      <td>7.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>vacation</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3660.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-01</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.43</td>\n",
       "      <td>155.38</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6053.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-01</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.28</td>\n",
       "      <td>97.20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-08-01</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.75</td>\n",
       "      <td>124.89</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>61800.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>other</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loan_amnt  term  int_rate  installment  emp_length home_ownership  \\\n",
       "issue_d                                                                         \n",
       "2007-07-01     3500.0    36     10.28       113.39         NaN           RENT   \n",
       "2007-07-01     5000.0    36      7.43       155.38         7.0       MORTGAGE   \n",
       "2007-07-01     5000.0    36      7.43       155.38         8.0       MORTGAGE   \n",
       "2007-07-01     3000.0    36     10.28        97.20         2.0       MORTGAGE   \n",
       "2007-08-01     4000.0    36      7.75       124.89         1.0            OWN   \n",
       "\n",
       "            annual_inc verification_status           purpose   dti  \\\n",
       "issue_d                                                              \n",
       "2007-07-01     20000.0        Not Verified            moving  1.50   \n",
       "2007-07-01     95000.0        Not Verified          vacation  3.83   \n",
       "2007-07-01    150000.0        Not Verified  home_improvement  0.00   \n",
       "2007-07-01    200000.0        Not Verified  home_improvement  0.00   \n",
       "2007-08-01     61800.0        Not Verified             other  3.46   \n",
       "\n",
       "            delinq_2yrs  fico_range_high  inq_last_6mths  open_acc  pub_rec  \\\n",
       "issue_d                                                                       \n",
       "2007-07-01          0.0            684.0             0.0      17.0      0.0   \n",
       "2007-07-01          0.0            774.0             0.0       8.0      0.0   \n",
       "2007-07-01          0.0            779.0             0.0       2.0      0.0   \n",
       "2007-07-01          1.0            679.0             0.0       5.0      0.0   \n",
       "2007-08-01          0.0            749.0             0.0      12.0      0.0   \n",
       "\n",
       "            revol_bal  revol_util  total_acc  application_type  \\\n",
       "issue_d                                                          \n",
       "2007-07-01     1882.0        32.4       18.0                 1   \n",
       "2007-07-01     3660.0         6.8       16.0                 1   \n",
       "2007-07-01     6053.0        19.5       19.0                 1   \n",
       "2007-07-01        0.0         0.0        8.0                 1   \n",
       "2007-08-01     1918.0        17.1       16.0                 1   \n",
       "\n",
       "            acc_now_delinq  tot_coll_amt  tot_cur_bal  total_rev_hi_lim  \\\n",
       "issue_d                                                                   \n",
       "2007-07-01             0.0           NaN          NaN               NaN   \n",
       "2007-07-01             0.0           NaN          NaN               NaN   \n",
       "2007-07-01             0.0           NaN          NaN               NaN   \n",
       "2007-07-01             0.0           NaN          NaN               NaN   \n",
       "2007-08-01             0.0           NaN          NaN               NaN   \n",
       "\n",
       "            acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "issue_d                                                                  \n",
       "2007-07-01                   NaN          NaN             NaN      NaN   \n",
       "2007-07-01                   NaN          NaN             NaN      NaN   \n",
       "2007-07-01                   NaN          NaN             NaN      NaN   \n",
       "2007-07-01                   NaN          NaN             NaN      NaN   \n",
       "2007-08-01                   NaN          NaN             NaN      NaN   \n",
       "\n",
       "            chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  \\\n",
       "issue_d                                                                 \n",
       "2007-07-01                       NaN          0.0                 NaN   \n",
       "2007-07-01                       NaN          0.0                 NaN   \n",
       "2007-07-01                       NaN          0.0                 NaN   \n",
       "2007-07-01                       NaN          0.0                 NaN   \n",
       "2007-08-01                       0.0          0.0                 NaN   \n",
       "\n",
       "            mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  \\\n",
       "issue_d                                                                   \n",
       "2007-07-01                   NaN                    NaN             NaN   \n",
       "2007-07-01                   NaN                    NaN             NaN   \n",
       "2007-07-01                   NaN                    NaN             NaN   \n",
       "2007-07-01                   NaN                    NaN             NaN   \n",
       "2007-08-01                   NaN                    NaN             NaN   \n",
       "\n",
       "            mort_acc  mths_since_recent_bc  mths_since_recent_inq  \\\n",
       "issue_d                                                             \n",
       "2007-07-01       NaN                   NaN                    NaN   \n",
       "2007-07-01       NaN                   NaN                    NaN   \n",
       "2007-07-01       NaN                   NaN                    NaN   \n",
       "2007-07-01       NaN                   NaN                    NaN   \n",
       "2007-08-01       NaN                   NaN                    NaN   \n",
       "\n",
       "            num_accts_ever_120_pd  num_actv_bc_tl  num_actv_rev_tl  \\\n",
       "issue_d                                                              \n",
       "2007-07-01                    NaN             NaN              NaN   \n",
       "2007-07-01                    NaN             NaN              NaN   \n",
       "2007-07-01                    NaN             NaN              NaN   \n",
       "2007-07-01                    NaN             NaN              NaN   \n",
       "2007-08-01                    NaN             NaN              NaN   \n",
       "\n",
       "            num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  \\\n",
       "issue_d                                                                       \n",
       "2007-07-01          NaN        NaN        NaN            NaN            NaN   \n",
       "2007-07-01          NaN        NaN        NaN            NaN            NaN   \n",
       "2007-07-01          NaN        NaN        NaN            NaN            NaN   \n",
       "2007-07-01          NaN        NaN        NaN            NaN            NaN   \n",
       "2007-08-01          NaN        NaN        NaN            NaN            NaN   \n",
       "\n",
       "            num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
       "issue_d                                                                     \n",
       "2007-07-01                  NaN       NaN               NaN           NaN   \n",
       "2007-07-01                  NaN       NaN               NaN           NaN   \n",
       "2007-07-01                  NaN       NaN               NaN           NaN   \n",
       "2007-07-01                  NaN       NaN               NaN           NaN   \n",
       "2007-08-01                  NaN       NaN               NaN           NaN   \n",
       "\n",
       "            num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "issue_d                                                              \n",
       "2007-07-01                 NaN                 NaN             NaN   \n",
       "2007-07-01                 NaN                 NaN             NaN   \n",
       "2007-07-01                 NaN                 NaN             NaN   \n",
       "2007-07-01                 NaN                 NaN             NaN   \n",
       "2007-08-01                 NaN                 NaN             NaN   \n",
       "\n",
       "            percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "issue_d                                                         \n",
       "2007-07-01               NaN                   NaN        0.0   \n",
       "2007-07-01               NaN                   NaN        NaN   \n",
       "2007-07-01               NaN                   NaN        NaN   \n",
       "2007-07-01               NaN                   NaN        NaN   \n",
       "2007-08-01               NaN                   NaN        0.0   \n",
       "\n",
       "            tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "issue_d                                                          \n",
       "2007-07-01              NaN                NaN             NaN   \n",
       "2007-07-01              NaN                NaN             NaN   \n",
       "2007-07-01              NaN                NaN             NaN   \n",
       "2007-07-01              NaN                NaN             NaN   \n",
       "2007-08-01              NaN                NaN             NaN   \n",
       "\n",
       "            total_il_high_credit_limit  credit_age  \n",
       "issue_d                                             \n",
       "2007-07-01                         NaN        1399  \n",
       "2007-07-01                         NaN        2556  \n",
       "2007-07-01                         NaN        7152  \n",
       "2007-07-01                         NaN        3802  \n",
       "2007-08-01                         NaN        2525  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape and head of dataframe\n",
    "print('Shape is: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4b0ef7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>110332.000000</td>\n",
       "      <td>1.283340e+05</td>\n",
       "      <td>128283.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128251.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>1.222600e+05</td>\n",
       "      <td>1.222600e+05</td>\n",
       "      <td>1.222600e+05</td>\n",
       "      <td>124049.000000</td>\n",
       "      <td>122259.000000</td>\n",
       "      <td>122813.000000</td>\n",
       "      <td>122748.000000</td>\n",
       "      <td>128328.000000</td>\n",
       "      <td>128334.000000</td>\n",
       "      <td>118606.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>124049.000000</td>\n",
       "      <td>122889.000000</td>\n",
       "      <td>111181.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>123290.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>123290.000000</td>\n",
       "      <td>117490.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122260.000000</td>\n",
       "      <td>122249.000000</td>\n",
       "      <td>122777.000000</td>\n",
       "      <td>128270.000000</td>\n",
       "      <td>128330.000000</td>\n",
       "      <td>1.222600e+05</td>\n",
       "      <td>1.240490e+05</td>\n",
       "      <td>1.240490e+05</td>\n",
       "      <td>1.222600e+05</td>\n",
       "      <td>128334.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14086.224227</td>\n",
       "      <td>40.738510</td>\n",
       "      <td>12.197903</td>\n",
       "      <td>427.166154</td>\n",
       "      <td>6.515336</td>\n",
       "      <td>7.701800e+04</td>\n",
       "      <td>17.992837</td>\n",
       "      <td>0.311702</td>\n",
       "      <td>702.102561</td>\n",
       "      <td>0.606457</td>\n",
       "      <td>11.566981</td>\n",
       "      <td>0.212126</td>\n",
       "      <td>16170.668755</td>\n",
       "      <td>50.735259</td>\n",
       "      <td>24.911676</td>\n",
       "      <td>0.978556</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>3.046517e+02</td>\n",
       "      <td>1.425918e+05</td>\n",
       "      <td>3.346212e+04</td>\n",
       "      <td>4.566502</td>\n",
       "      <td>13672.122380</td>\n",
       "      <td>10808.781351</td>\n",
       "      <td>58.705261</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>15.151978</td>\n",
       "      <td>126.718851</td>\n",
       "      <td>183.661917</td>\n",
       "      <td>13.440733</td>\n",
       "      <td>8.060420</td>\n",
       "      <td>1.674492</td>\n",
       "      <td>24.339127</td>\n",
       "      <td>6.948274</td>\n",
       "      <td>0.511369</td>\n",
       "      <td>3.633527</td>\n",
       "      <td>5.591076</td>\n",
       "      <td>4.749882</td>\n",
       "      <td>8.103869</td>\n",
       "      <td>8.514297</td>\n",
       "      <td>8.259038</td>\n",
       "      <td>14.583748</td>\n",
       "      <td>5.541027</td>\n",
       "      <td>11.610609</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.003321</td>\n",
       "      <td>0.087428</td>\n",
       "      <td>2.096221</td>\n",
       "      <td>94.150099</td>\n",
       "      <td>43.522208</td>\n",
       "      <td>0.132743</td>\n",
       "      <td>0.051742</td>\n",
       "      <td>1.769755e+05</td>\n",
       "      <td>4.945455e+04</td>\n",
       "      <td>2.230894e+04</td>\n",
       "      <td>4.213447e+04</td>\n",
       "      <td>6006.686077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8614.852963</td>\n",
       "      <td>9.553611</td>\n",
       "      <td>3.703500</td>\n",
       "      <td>257.585471</td>\n",
       "      <td>3.357604</td>\n",
       "      <td>7.953183e+04</td>\n",
       "      <td>10.661602</td>\n",
       "      <td>0.870466</td>\n",
       "      <td>32.595891</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>5.453600</td>\n",
       "      <td>0.614769</td>\n",
       "      <td>21340.637088</td>\n",
       "      <td>24.440515</td>\n",
       "      <td>11.901112</td>\n",
       "      <td>0.144860</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>2.627026e+04</td>\n",
       "      <td>1.586927e+05</td>\n",
       "      <td>3.318267e+04</td>\n",
       "      <td>3.114648</td>\n",
       "      <td>16549.896057</td>\n",
       "      <td>15937.004094</td>\n",
       "      <td>28.261167</td>\n",
       "      <td>0.106211</td>\n",
       "      <td>798.333509</td>\n",
       "      <td>52.265588</td>\n",
       "      <td>95.398123</td>\n",
       "      <td>16.539138</td>\n",
       "      <td>8.765441</td>\n",
       "      <td>1.998014</td>\n",
       "      <td>31.466584</td>\n",
       "      <td>5.907532</td>\n",
       "      <td>1.351877</td>\n",
       "      <td>2.231473</td>\n",
       "      <td>3.259552</td>\n",
       "      <td>2.949345</td>\n",
       "      <td>4.779161</td>\n",
       "      <td>7.305871</td>\n",
       "      <td>4.533252</td>\n",
       "      <td>8.066640</td>\n",
       "      <td>3.176103</td>\n",
       "      <td>5.468192</td>\n",
       "      <td>0.029310</td>\n",
       "      <td>0.060171</td>\n",
       "      <td>0.495908</td>\n",
       "      <td>1.788898</td>\n",
       "      <td>8.829109</td>\n",
       "      <td>35.850810</td>\n",
       "      <td>0.373823</td>\n",
       "      <td>0.426985</td>\n",
       "      <td>1.795068e+05</td>\n",
       "      <td>4.746393e+04</td>\n",
       "      <td>2.227299e+04</td>\n",
       "      <td>4.389137e+04</td>\n",
       "      <td>2776.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>5.310000</td>\n",
       "      <td>14.770000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>664.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1096.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7500.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>9.170000</td>\n",
       "      <td>240.920000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.600000e+04</td>\n",
       "      <td>11.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>679.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5935.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.919900e+04</td>\n",
       "      <td>1.430000e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3097.500000</td>\n",
       "      <td>1630.000000</td>\n",
       "      <td>36.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.011500e+04</td>\n",
       "      <td>2.050700e+04</td>\n",
       "      <td>8.000000e+03</td>\n",
       "      <td>1.450700e+04</td>\n",
       "      <td>4139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12000.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>12.120000</td>\n",
       "      <td>363.970000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>17.310000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11081.000000</td>\n",
       "      <td>50.900000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.261300e+04</td>\n",
       "      <td>2.460000e+04</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7595.000000</td>\n",
       "      <td>5105.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.158875e+05</td>\n",
       "      <td>3.703300e+04</td>\n",
       "      <td>1.560000e+04</td>\n",
       "      <td>3.157800e+04</td>\n",
       "      <td>5447.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>14.650000</td>\n",
       "      <td>561.667500</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.200000e+04</td>\n",
       "      <td>23.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19755.000000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.128252e+05</td>\n",
       "      <td>4.160000e+04</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>19056.000000</td>\n",
       "      <td>13307.000000</td>\n",
       "      <td>83.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.553320e+05</td>\n",
       "      <td>6.246000e+04</td>\n",
       "      <td>2.900000e+04</td>\n",
       "      <td>5.662900e+04</td>\n",
       "      <td>7397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40000.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>28.800000</td>\n",
       "      <td>1524.520000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.300000e+06</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>959754.000000</td>\n",
       "      <td>150.700000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.152545e+06</td>\n",
       "      <td>4.151547e+06</td>\n",
       "      <td>1.184500e+06</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>958084.000000</td>\n",
       "      <td>497445.000000</td>\n",
       "      <td>187.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>86399.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>781.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>9.999999e+06</td>\n",
       "      <td>1.548128e+06</td>\n",
       "      <td>1.105500e+06</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>23773.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loan_amnt           term       int_rate    installment  \\\n",
       "count  128334.000000  128334.000000  128334.000000  128334.000000   \n",
       "mean    14086.224227      40.738510      12.197903     427.166154   \n",
       "std      8614.852963       9.553611       3.703500     257.585471   \n",
       "min       500.000000      36.000000       5.310000      14.770000   \n",
       "25%      7500.000000      36.000000       9.170000     240.920000   \n",
       "50%     12000.000000      36.000000      12.120000     363.970000   \n",
       "75%     20000.000000      36.000000      14.650000     561.667500   \n",
       "max     40000.000000      60.000000      28.800000    1524.520000   \n",
       "\n",
       "          emp_length    annual_inc            dti    delinq_2yrs  \\\n",
       "count  110332.000000  1.283340e+05  128283.000000  128334.000000   \n",
       "mean        6.515336  7.701800e+04      17.992837       0.311702   \n",
       "std         3.357604  7.953183e+04      10.661602       0.870466   \n",
       "min         1.000000  0.000000e+00       0.000000       0.000000   \n",
       "25%         3.000000  4.600000e+04      11.570000       0.000000   \n",
       "50%         7.000000  6.500000e+04      17.310000       0.000000   \n",
       "75%        10.000000  9.200000e+04      23.670000       0.000000   \n",
       "max        10.000000  9.300000e+06     999.000000      24.000000   \n",
       "\n",
       "       fico_range_high  inq_last_6mths       open_acc        pub_rec  \\\n",
       "count    128334.000000   128334.000000  128334.000000  128334.000000   \n",
       "mean        702.102561        0.606457      11.566981       0.212126   \n",
       "std          32.595891        0.897196       5.453600       0.614769   \n",
       "min         664.000000        0.000000       1.000000       0.000000   \n",
       "25%         679.000000        0.000000       8.000000       0.000000   \n",
       "50%         694.000000        0.000000      11.000000       0.000000   \n",
       "75%         719.000000        1.000000      14.000000       0.000000   \n",
       "max         850.000000        8.000000      67.000000      61.000000   \n",
       "\n",
       "           revol_bal     revol_util      total_acc  application_type  \\\n",
       "count  128334.000000  128251.000000  128334.000000     128334.000000   \n",
       "mean    16170.668755      50.735259      24.911676          0.978556   \n",
       "std     21340.637088      24.440515      11.901112          0.144860   \n",
       "min         0.000000       0.000000       2.000000          0.000000   \n",
       "25%      5935.000000      32.300000      16.000000          1.000000   \n",
       "50%     11081.000000      50.900000      23.000000          1.000000   \n",
       "75%     19755.000000      69.500000      32.000000          1.000000   \n",
       "max    959754.000000     150.700000     142.000000          1.000000   \n",
       "\n",
       "       acc_now_delinq  tot_coll_amt   tot_cur_bal  total_rev_hi_lim  \\\n",
       "count   128334.000000  1.222600e+05  1.222600e+05      1.222600e+05   \n",
       "mean         0.004761  3.046517e+02  1.425918e+05      3.346212e+04   \n",
       "std          0.072904  2.627026e+04  1.586927e+05      3.318267e+04   \n",
       "min          0.000000  0.000000e+00  0.000000e+00      0.000000e+00   \n",
       "25%          0.000000  0.000000e+00  2.919900e+04      1.430000e+04   \n",
       "50%          0.000000  0.000000e+00  8.261300e+04      2.460000e+04   \n",
       "75%          0.000000  0.000000e+00  2.128252e+05      4.160000e+04   \n",
       "max          4.000000  9.152545e+06  4.151547e+06      1.184500e+06   \n",
       "\n",
       "       acc_open_past_24mths    avg_cur_bal  bc_open_to_buy        bc_util  \\\n",
       "count         124049.000000  122259.000000   122813.000000  122748.000000   \n",
       "mean               4.566502   13672.122380    10808.781351      58.705261   \n",
       "std                3.114648   16549.896057    15937.004094      28.261167   \n",
       "min                0.000000       0.000000        0.000000       0.000000   \n",
       "25%                2.000000    3097.500000     1630.000000      36.900000   \n",
       "50%                4.000000    7595.000000     5105.000000      61.500000   \n",
       "75%                6.000000   19056.000000    13307.000000      83.400000   \n",
       "max               42.000000  958084.000000   497445.000000     187.900000   \n",
       "\n",
       "       chargeoff_within_12_mths    delinq_amnt  mo_sin_old_il_acct  \\\n",
       "count             128328.000000  128334.000000       118606.000000   \n",
       "mean                   0.008993      15.151978          126.718851   \n",
       "std                    0.106211     798.333509           52.265588   \n",
       "min                    0.000000       0.000000            0.000000   \n",
       "25%                    0.000000       0.000000           99.000000   \n",
       "50%                    0.000000       0.000000          130.000000   \n",
       "75%                    0.000000       0.000000          153.000000   \n",
       "max                    5.000000   86399.000000          507.000000   \n",
       "\n",
       "       mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  \\\n",
       "count         122260.000000          122260.000000   122260.000000   \n",
       "mean             183.661917              13.440733        8.060420   \n",
       "std               95.398123              16.539138        8.765441   \n",
       "min                3.000000               0.000000        0.000000   \n",
       "25%              119.000000               4.000000        3.000000   \n",
       "50%              166.000000               8.000000        6.000000   \n",
       "75%              233.000000              16.000000       10.000000   \n",
       "max              781.000000             315.000000      194.000000   \n",
       "\n",
       "            mort_acc  mths_since_recent_bc  mths_since_recent_inq  \\\n",
       "count  124049.000000         122889.000000          111181.000000   \n",
       "mean        1.674492             24.339127               6.948274   \n",
       "std         1.998014             31.466584               5.907532   \n",
       "min         0.000000              0.000000               0.000000   \n",
       "25%         0.000000              6.000000               2.000000   \n",
       "50%         1.000000             14.000000               5.000000   \n",
       "75%         3.000000             29.000000              10.000000   \n",
       "max        31.000000            577.000000              25.000000   \n",
       "\n",
       "       num_accts_ever_120_pd  num_actv_bc_tl  num_actv_rev_tl    num_bc_sats  \\\n",
       "count          122260.000000   122260.000000    122260.000000  123290.000000   \n",
       "mean                0.511369        3.633527         5.591076       4.749882   \n",
       "std                 1.351877        2.231473         3.259552       2.949345   \n",
       "min                 0.000000        0.000000         0.000000       0.000000   \n",
       "25%                 0.000000        2.000000         3.000000       3.000000   \n",
       "50%                 0.000000        3.000000         5.000000       4.000000   \n",
       "75%                 0.000000        5.000000         7.000000       6.000000   \n",
       "max                34.000000       26.000000        46.000000      42.000000   \n",
       "\n",
       "           num_bc_tl      num_il_tl  num_op_rev_tl  num_rev_accts  \\\n",
       "count  122260.000000  122260.000000  122260.000000  122260.000000   \n",
       "mean        8.103869       8.514297       8.259038      14.583748   \n",
       "std         4.779161       7.305871       4.533252       8.066640   \n",
       "min         0.000000       0.000000       0.000000       1.000000   \n",
       "25%         5.000000       3.000000       5.000000       9.000000   \n",
       "50%         7.000000       7.000000       7.000000      13.000000   \n",
       "75%        11.000000      11.000000      10.000000      19.000000   \n",
       "max        61.000000      91.000000      64.000000     112.000000   \n",
       "\n",
       "       num_rev_tl_bal_gt_0       num_sats  num_tl_120dpd_2m   num_tl_30dpd  \\\n",
       "count        122260.000000  123290.000000     117490.000000  122260.000000   \n",
       "mean              5.541027      11.610609          0.000775       0.003321   \n",
       "std               3.176103       5.468192          0.029310       0.060171   \n",
       "min               0.000000       1.000000          0.000000       0.000000   \n",
       "25%               3.000000       8.000000          0.000000       0.000000   \n",
       "50%               5.000000      11.000000          0.000000       0.000000   \n",
       "75%               7.000000      14.000000          0.000000       0.000000   \n",
       "max              35.000000      67.000000          3.000000       3.000000   \n",
       "\n",
       "       num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "count       122260.000000       122260.000000   122249.000000   \n",
       "mean             0.087428            2.096221       94.150099   \n",
       "std              0.495908            1.788898        8.829109   \n",
       "min              0.000000            0.000000       16.700000   \n",
       "25%              0.000000            1.000000       91.300000   \n",
       "50%              0.000000            2.000000       98.000000   \n",
       "75%              0.000000            3.000000      100.000000   \n",
       "max             24.000000           29.000000      100.000000   \n",
       "\n",
       "       percent_bc_gt_75  pub_rec_bankruptcies      tax_liens  tot_hi_cred_lim  \\\n",
       "count     122777.000000         128270.000000  128330.000000     1.222600e+05   \n",
       "mean          43.522208              0.132743       0.051742     1.769755e+05   \n",
       "std           35.850810              0.373823       0.426985     1.795068e+05   \n",
       "min            0.000000              0.000000       0.000000     0.000000e+00   \n",
       "25%            0.000000              0.000000       0.000000     5.011500e+04   \n",
       "50%           40.000000              0.000000       0.000000     1.158875e+05   \n",
       "75%           75.000000              0.000000       0.000000     2.553320e+05   \n",
       "max          100.000000              8.000000      61.000000     9.999999e+06   \n",
       "\n",
       "       total_bal_ex_mort  total_bc_limit  total_il_high_credit_limit  \\\n",
       "count       1.240490e+05    1.240490e+05                1.222600e+05   \n",
       "mean        4.945455e+04    2.230894e+04                4.213447e+04   \n",
       "std         4.746393e+04    2.227299e+04                4.389137e+04   \n",
       "min         0.000000e+00    0.000000e+00                0.000000e+00   \n",
       "25%         2.050700e+04    8.000000e+03                1.450700e+04   \n",
       "50%         3.703300e+04    1.560000e+04                3.157800e+04   \n",
       "75%         6.246000e+04    2.900000e+04                5.662900e+04   \n",
       "max         1.548128e+06    1.105500e+06                2.000000e+06   \n",
       "\n",
       "          credit_age  \n",
       "count  128334.000000  \n",
       "mean     6006.686077  \n",
       "std      2776.741935  \n",
       "min      1096.000000  \n",
       "25%      4139.000000  \n",
       "50%      5447.000000  \n",
       "75%      7397.000000  \n",
       "max     23773.000000  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9a6d4",
   "metadata": {},
   "source": [
    "# Creating Training and Validation Data\n",
    "Here, I create the X (predictor) and y (target) matrices which will then be split into training and testing sets for my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ecec5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_d</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-07-01</th>\n",
       "      <td>3500.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.28</td>\n",
       "      <td>113.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENT</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>moving</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1882.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loan_amnt  term  int_rate  installment  emp_length home_ownership  \\\n",
       "issue_d                                                                         \n",
       "2007-07-01     3500.0    36     10.28       113.39         NaN           RENT   \n",
       "\n",
       "            annual_inc verification_status purpose  dti  delinq_2yrs  \\\n",
       "issue_d                                                                \n",
       "2007-07-01     20000.0        Not Verified  moving  1.5          0.0   \n",
       "\n",
       "            fico_range_high  inq_last_6mths  open_acc  pub_rec  revol_bal  \\\n",
       "issue_d                                                                     \n",
       "2007-07-01            684.0             0.0      17.0      0.0     1882.0   \n",
       "\n",
       "            revol_util  total_acc  application_type  acc_now_delinq  \\\n",
       "issue_d                                                               \n",
       "2007-07-01        32.4       18.0                 1             0.0   \n",
       "\n",
       "            tot_coll_amt  tot_cur_bal  total_rev_hi_lim  acc_open_past_24mths  \\\n",
       "issue_d                                                                         \n",
       "2007-07-01           NaN          NaN               NaN                   NaN   \n",
       "\n",
       "            avg_cur_bal  bc_open_to_buy  bc_util  chargeoff_within_12_mths  \\\n",
       "issue_d                                                                      \n",
       "2007-07-01          NaN             NaN      NaN                       NaN   \n",
       "\n",
       "            delinq_amnt  mo_sin_old_il_acct  mo_sin_old_rev_tl_op  \\\n",
       "issue_d                                                             \n",
       "2007-07-01          0.0                 NaN                   NaN   \n",
       "\n",
       "            mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  \\\n",
       "issue_d                                                       \n",
       "2007-07-01                    NaN             NaN       NaN   \n",
       "\n",
       "            mths_since_recent_bc  mths_since_recent_inq  \\\n",
       "issue_d                                                   \n",
       "2007-07-01                   NaN                    NaN   \n",
       "\n",
       "            num_accts_ever_120_pd  num_actv_bc_tl  num_actv_rev_tl  \\\n",
       "issue_d                                                              \n",
       "2007-07-01                    NaN             NaN              NaN   \n",
       "\n",
       "            num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  \\\n",
       "issue_d                                                                       \n",
       "2007-07-01          NaN        NaN        NaN            NaN            NaN   \n",
       "\n",
       "            num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
       "issue_d                                                                     \n",
       "2007-07-01                  NaN       NaN               NaN           NaN   \n",
       "\n",
       "            num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "issue_d                                                              \n",
       "2007-07-01                 NaN                 NaN             NaN   \n",
       "\n",
       "            percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "issue_d                                                         \n",
       "2007-07-01               NaN                   NaN        0.0   \n",
       "\n",
       "            tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "issue_d                                                          \n",
       "2007-07-01              NaN                NaN             NaN   \n",
       "\n",
       "            total_il_high_credit_limit  credit_age  \n",
       "issue_d                                             \n",
       "2007-07-01                         NaN        1399  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_d</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>4200.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.49</td>\n",
       "      <td>138.48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>44000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2139.0</td>\n",
       "      <td>22.3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5496.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>5051.0</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16095.0</td>\n",
       "      <td>5496.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>6495.0</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loan_amnt  term  int_rate  installment  emp_length home_ownership  \\\n",
       "issue_d                                                                         \n",
       "2016-12-01     4200.0    36     11.49       138.48         3.0           RENT   \n",
       "\n",
       "            annual_inc verification_status             purpose   dti  \\\n",
       "issue_d                                                                \n",
       "2016-12-01     44000.0        Not Verified  debt_consolidation  9.08   \n",
       "\n",
       "            delinq_2yrs  fico_range_high  inq_last_6mths  open_acc  pub_rec  \\\n",
       "issue_d                                                                       \n",
       "2016-12-01          0.0            669.0             2.0      10.0      1.0   \n",
       "\n",
       "            revol_bal  revol_util  total_acc  application_type  \\\n",
       "issue_d                                                          \n",
       "2016-12-01     2139.0        22.3       34.0                 1   \n",
       "\n",
       "            acc_now_delinq  tot_coll_amt  tot_cur_bal  total_rev_hi_lim  \\\n",
       "issue_d                                                                   \n",
       "2016-12-01             0.0           0.0       5496.0            9600.0   \n",
       "\n",
       "            acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "issue_d                                                                  \n",
       "2016-12-01                   8.0        611.0          5051.0     27.8   \n",
       "\n",
       "            chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  \\\n",
       "issue_d                                                                 \n",
       "2016-12-01                       0.0          0.0               165.0   \n",
       "\n",
       "            mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  \\\n",
       "issue_d                                                                   \n",
       "2016-12-01                 169.0                    5.0             5.0   \n",
       "\n",
       "            mort_acc  mths_since_recent_bc  mths_since_recent_inq  \\\n",
       "issue_d                                                             \n",
       "2016-12-01       0.0                  17.0                    5.0   \n",
       "\n",
       "            num_accts_ever_120_pd  num_actv_bc_tl  num_actv_rev_tl  \\\n",
       "issue_d                                                              \n",
       "2016-12-01                    0.0             2.0              5.0   \n",
       "\n",
       "            num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  num_rev_accts  \\\n",
       "issue_d                                                                       \n",
       "2016-12-01          2.0       14.0        4.0            7.0           27.0   \n",
       "\n",
       "            num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  num_tl_30dpd  \\\n",
       "issue_d                                                                     \n",
       "2016-12-01                  5.0       9.0               0.0           0.0   \n",
       "\n",
       "            num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "issue_d                                                              \n",
       "2016-12-01                 0.0                 3.0           100.0   \n",
       "\n",
       "            percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "issue_d                                                         \n",
       "2016-12-01               0.0                   1.0        0.0   \n",
       "\n",
       "            tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "issue_d                                                          \n",
       "2016-12-01          16095.0             5496.0          7000.0   \n",
       "\n",
       "            total_il_high_credit_limit  credit_age  \n",
       "issue_d                                             \n",
       "2016-12-01                      6495.0        5144  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we want to have ~80% of our data for training and ~20% for validation\n",
    "# we see that the 80th percentile is December 2016\n",
    "# we have a very large dataset, so we choose to start in January 2017\n",
    "display(df.iloc[0:1], df.iloc[round(df.shape[0]*0.8):round(df.shape[0]*0.8)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02382a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our prediction target is the interest rate, our predictive matrix will be everything except the target\n",
    "target = 'int_rate'\n",
    "y = df[target]\n",
    "X = df.drop(columns = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "289fa415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is (103649, 58) for X and (103649,) for y.\n",
      "The shape of the validation data is (24685, 58) for X and (24685,) for y.\n"
     ]
    }
   ],
   "source": [
    "# separate our training and validation data by our cutoff of January 2017\n",
    "cutoff = '2017-01-01'\n",
    "mask = X.index < cutoff\n",
    "X_train, y_train = X[mask], y[mask]\n",
    "X_val, y_val = X[~mask], y[~mask]\n",
    "\n",
    "# check shapes to make sure dimensions are correct\n",
    "print(f'The shape of the training data is {X_train.shape} for X and {y_train.shape} for y.')\n",
    "print(f'The shape of the validation data is {X_val.shape} for X and {y_val.shape} for y.')\n",
    "\n",
    "# make sure no data was lost in the splice\n",
    "assert len(X_train) + len(X_val) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e5a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 3.360100268400727\n",
      "Baseline R2: -0.009930713374478373\n"
     ]
    }
   ],
   "source": [
    "# our baseline mean absolute error will be the mean interest rate\n",
    "# baseline R^2 score will of course be 0 or approximately 0\n",
    "baseline_reg = [y_train.mean()]*len(y_val)\n",
    "print('Baseline MAE:', mean_absolute_error(y_val, baseline_reg))\n",
    "print('Baseline R2:', r2_score(y_val, baseline_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551cd91",
   "metadata": {},
   "source": [
    "# Best Model - XGBoost \n",
    "For the sake of the reader's time, I include my final model here so that it is not necessary to scroll through the various models and tuning of hyperparameters.\n",
    "\n",
    "My final model predicts interest rate with a mean error below 0.9% and a cross-validated R^2 of 0.91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f99a5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.34496464560571966\n",
      "Validation MAE: 0.8994065691179319\n",
      "Validation R2: 0.9022665324154702\n",
      "CPU times: user 2min 55s, sys: 2.1 s, total: 2min 58s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_final = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                           StandardScaler(),\n",
    "                           SimpleImputer(strategy='mean'),\n",
    "                           XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_alpha = 0,\n",
    "                                        reg_lambda = 1,\n",
    "                                        gamma = 0))\n",
    "model_final.fit(X_train, y_train)\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_final.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_final.predict(X_val)))\n",
    "print('Validation R2:', model_boost.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c4f9039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.87971062 0.92950602 0.91982579 0.93255404 0.90019554]\n",
      "Mean: 0.9123584023146762\n"
     ]
    }
   ],
   "source": [
    "cross_vals = cross_val_score(model_final, X, y)\n",
    "print(f'Scores: {cross_vals}\\nMean: {np.mean(cross_vals)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8fedb",
   "metadata": {},
   "source": [
    "# On Optimizing Hyperparameters\n",
    "For each model, I will optimize whatever hyperparameters are available for that model.\n",
    "\n",
    "In testing the hyperparameters, I will look at the Mean Absolute Error and R^2 (what percentage of the variance in the data my model accounts for) for the training and validation data, as well as the time it took - the time is necessary since when testing each hyperparameter I do not necessarily want the previously tested hyperparameters to be optimal since the model may take too much time.\n",
    "\n",
    "By having the MAE, R^2, and Time for each hyperparameter I can know which hyperparameter I will use in my final model while also learning which hyperparameter will be suitable for tuning other hyperparameters - i.e. for my final model I will use the hyperparameter with the lowest MAE and highest R^2, but when tuning the other hyperparameters I will use a value which has a MAE and R^2 *similar* to the best but not as time-expensive.\n",
    "\n",
    "I choose not to use a GridSearch for the hyperparameters since I am attempting to optimize every hyperparameter and the exponential increase in the time it would take to train my model is not worth the small amount of additional certainty I would gain from knowing I've found the absolute best combination - also, I am sure from past experience with exactly this problem that the increase in accuracy from a GridSearch would be very small compared to the overall increase in accuracy gained through my method (which itself is still small compared to the accuracy of the model itself). \n",
    "\n",
    "In going through each model, I also choose the order in which I optimize hyperparameters very deliberately. I start with hyperparameters that will have the greatest effect on the accuracy and then move to hyperparameters with smaller effects - in essence, ordering them so that the later hyperparameters do not affect the earlier hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417d2d4",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor\n",
    "Here, I will test the decision tree model and optimize hyperparameters. It is very unlikely that a decision tree will be the best model, as we naturally expect any random forest to outcompete a single tree.\n",
    "\n",
    "We find, after optimizing hyperparameters, the MAE is 2.28% and the R^2 is 0.4\n",
    "\n",
    "The only significant hyperparameter to optimize for a decision tree is the depth of the tree - since we have a large dataset, we can rely on the algorithm and not have to choose the minimum samples for a split or a leaf, the maximum features or leaves, or the minimum decrease in impurity. \n",
    "\n",
    "We see the depth of the tree significantly affects the accuracy and R^2 but the split criteria does not, and also that it is wise to stick with the default \"best\" splitter and not the \"random\" splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb563286",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.748390</td>\n",
       "      <td>3.051541</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.141579</td>\n",
       "      <td>2.677304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.383164</td>\n",
       "      <td>2.682336</td>\n",
       "      <td>0.323038</td>\n",
       "      <td>0.293088</td>\n",
       "      <td>3.093533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.073969</td>\n",
       "      <td>2.448617</td>\n",
       "      <td>0.458468</td>\n",
       "      <td>0.372393</td>\n",
       "      <td>4.335521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.666364</td>\n",
       "      <td>2.278611</td>\n",
       "      <td>0.608790</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>5.271933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.094386</td>\n",
       "      <td>2.309931</td>\n",
       "      <td>0.784048</td>\n",
       "      <td>0.347419</td>\n",
       "      <td>5.716791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.522953</td>\n",
       "      <td>2.410450</td>\n",
       "      <td>0.916193</td>\n",
       "      <td>0.284530</td>\n",
       "      <td>6.458872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "1            2.748390             3.051541      0.135211        0.141579   \n",
       "4            2.383164             2.682336      0.323038        0.293088   \n",
       "8            2.073969             2.448617      0.458468        0.372393   \n",
       "12           1.666364             2.278611      0.608790        0.404045   \n",
       "16           1.094386             2.309931      0.784048        0.347419   \n",
       "20           0.522953             2.410450      0.916193        0.284530   \n",
       "\n",
       "        Time  \n",
       "1   2.677304  \n",
       "4   3.093533  \n",
       "8   4.335521  \n",
       "12  5.271933  \n",
       "16  5.716791  \n",
       "20  6.458872  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a broad search first then a narrow search next to save time\n",
    "depths = [1] + [i for i in range(4, 21, 4)]\n",
    "train_acc_dt = []\n",
    "val_acc_dt = []\n",
    "train_r2_dt = []\n",
    "val_r2_dt = []\n",
    "times = []\n",
    "\n",
    "# for each depth, train the model on the training data then add the accuracies, r^2, and time\n",
    "for depth in depths:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         DecisionTreeRegressor(random_state = 42,\n",
    "                                               max_depth = depth))\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc_dt.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_dt.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_dt.append(model.score(X_train, y_train))\n",
    "    val_r2_dt.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "# display stored values to determine best parameter\n",
    "pd.DataFrame(list(zip(train_acc_dt, val_acc_dt, train_r2_dt, val_r2_dt, times)), index = depths,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a09512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.073969</td>\n",
       "      <td>2.448617</td>\n",
       "      <td>0.458468</td>\n",
       "      <td>0.372393</td>\n",
       "      <td>4.554083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.980117</td>\n",
       "      <td>2.392809</td>\n",
       "      <td>0.492881</td>\n",
       "      <td>0.385546</td>\n",
       "      <td>4.591072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.890626</td>\n",
       "      <td>2.340187</td>\n",
       "      <td>0.528735</td>\n",
       "      <td>0.399359</td>\n",
       "      <td>4.740514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.781031</td>\n",
       "      <td>2.299303</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>0.405387</td>\n",
       "      <td>4.927080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.666364</td>\n",
       "      <td>2.278611</td>\n",
       "      <td>0.608790</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>5.348295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.535595</td>\n",
       "      <td>2.272576</td>\n",
       "      <td>0.652739</td>\n",
       "      <td>0.395549</td>\n",
       "      <td>5.567626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.396054</td>\n",
       "      <td>2.269537</td>\n",
       "      <td>0.697081</td>\n",
       "      <td>0.385290</td>\n",
       "      <td>5.706276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.246817</td>\n",
       "      <td>2.278888</td>\n",
       "      <td>0.741497</td>\n",
       "      <td>0.370015</td>\n",
       "      <td>5.826049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.094386</td>\n",
       "      <td>2.309931</td>\n",
       "      <td>0.784048</td>\n",
       "      <td>0.347419</td>\n",
       "      <td>6.126301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "8            2.073969             2.448617      0.458468        0.372393   \n",
       "9            1.980117             2.392809      0.492881        0.385546   \n",
       "10           1.890626             2.340187      0.528735        0.399359   \n",
       "11           1.781031             2.299303      0.567806        0.405387   \n",
       "12           1.666364             2.278611      0.608790        0.404045   \n",
       "13           1.535595             2.272576      0.652739        0.395549   \n",
       "14           1.396054             2.269537      0.697081        0.385290   \n",
       "15           1.246817             2.278888      0.741497        0.370015   \n",
       "16           1.094386             2.309931      0.784048        0.347419   \n",
       "\n",
       "        Time  \n",
       "8   4.554083  \n",
       "9   4.591072  \n",
       "10  4.740514  \n",
       "11  4.927080  \n",
       "12  5.348295  \n",
       "13  5.567626  \n",
       "14  5.706276  \n",
       "15  5.826049  \n",
       "16  6.126301  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time for the narrow search\n",
    "depths = range(8, 17)\n",
    "train_acc_dt = []\n",
    "val_acc_dt = []\n",
    "train_r2_dt = []\n",
    "val_r2_dt = []\n",
    "times = []\n",
    "\n",
    "# for each depth, train the model on the training data then add the accuracies, r^2, and time\n",
    "for depth in depths:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         DecisionTreeRegressor(random_state = 42,\n",
    "                                               max_depth = depth))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_dt.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_dt.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_dt.append(model.score(X_train, y_train))\n",
    "    val_r2_dt.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "# display stored values to determine best parameter\n",
    "pd.DataFrame(list(zip(train_acc_dt, val_acc_dt, train_r2_dt, val_r2_dt, times)), index = depths,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ddb528e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>1.666364</td>\n",
       "      <td>2.278611</td>\n",
       "      <td>0.60879</td>\n",
       "      <td>0.404045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friedman_mse</th>\n",
       "      <td>1.666365</td>\n",
       "      <td>2.282730</td>\n",
       "      <td>0.60879</td>\n",
       "      <td>0.402308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Training Accuracy  Validation Accuracy  Training R^2  \\\n",
       "mse                    1.666364             2.278611       0.60879   \n",
       "friedman_mse           1.666365             2.282730       0.60879   \n",
       "\n",
       "              Validation R^2  \n",
       "mse                 0.404045  \n",
       "friedman_mse        0.402308  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we decide if we should be using MSE or Friedman_MSE to split the cells in the tree - I ignore MAE\n",
    "# and reducing poisson deviance because the models take significantly longer to train than the MSE variants\n",
    "# and in attempting the model, my kernel crashes and must be restarted from the beginning :(\n",
    "criteria = [\"mse\", \"friedman_mse\"]\n",
    "train_acc_dt = []\n",
    "val_acc_dt = []\n",
    "train_r2_dt = []\n",
    "val_r2_dt = []\n",
    "\n",
    "for crit in criteria:\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         DecisionTreeRegressor(random_state = 42,\n",
    "                                               criterion = crit,\n",
    "                                               max_depth = 12))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_dt.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_dt.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_dt.append(model.score(X_train, y_train))\n",
    "    val_r2_dt.append(model.score(X_val, y_val))\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_dt, val_acc_dt, train_r2_dt, val_r2_dt)), index = criteria,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecfc241a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>1.666364</td>\n",
       "      <td>2.278611</td>\n",
       "      <td>0.608790</td>\n",
       "      <td>0.404045</td>\n",
       "      <td>5.090214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>1.998140</td>\n",
       "      <td>2.571510</td>\n",
       "      <td>0.497666</td>\n",
       "      <td>0.317036</td>\n",
       "      <td>3.094958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "best             1.666364             2.278611      0.608790        0.404045   \n",
       "random           1.998140             2.571510      0.497666        0.317036   \n",
       "\n",
       "            Time  \n",
       "best    5.090214  \n",
       "random  3.094958  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was curious how the 'random' splitter would compare to the 'best' - as expected, 'best' is much better\n",
    "splits = ['best', 'random']\n",
    "train_acc_dt = []\n",
    "val_acc_dt = []\n",
    "train_r2_dt = []\n",
    "val_r2_dt = []\n",
    "times = []\n",
    "\n",
    "for split in splits:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         DecisionTreeRegressor(random_state = 42,\n",
    "                                               splitter = split,\n",
    "                                               max_depth = 12))\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_dt.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_dt.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_dt.append(model.score(X_train, y_train))\n",
    "    val_r2_dt.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_dt, val_acc_dt, train_r2_dt, val_r2_dt, times)), index = splits,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec8833c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our decision tree model here\n",
    "model_dt = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                        StandardScaler(),\n",
    "                        SimpleImputer(strategy = 'mean'),\n",
    "                        DecisionTreeRegressor(random_state = 42,\n",
    "                                              max_depth = 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3be0557c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 1.6663641214606018\n",
      "Validation MAE: 2.2786111301924628\n",
      "Validation R2: 0.40404549101067366\n",
      "CPU times: user 4.29 s, sys: 563 ms, total: 4.86 s\n",
      "Wall time: 4.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dt.fit(X_train, y_train)\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_dt.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_dt.predict(X_val)))\n",
    "print('Validation R2:', model_dt.score(X_val,y_val))\n",
    "\n",
    "# our MAE isn't bad for a weak model, but we can do better. R^2 is very low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2e5773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Most Important Features'}>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here I graph the most important features in our model\n",
    "\n",
    "importances = model_dt.named_steps['decisiontreeregressor'].feature_importances_\n",
    "features = model_dt.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp_dt = pd.Series(importances, index = features,\n",
    "                       name = 'tree').abs().sort_values(ascending=False)\n",
    "feat_imp_dt.head(10).plot(kind = 'barh', title = 'Most Important Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3465c",
   "metadata": {},
   "source": [
    "# Making Tuning Dataset\n",
    "For the random forest model, my kernel kept crashing when optimizing hyperparameters by using the full dataset so I decide to tune the hyperparameters using a 'tuning' dataset which is 1/10 of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bcf8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10364, 58), (103649, 58)\n",
      "(10364,), (103649,)\n",
      "(2468, 58), (24685, 58)\n",
      "(2468,), (24685,)\n"
     ]
    }
   ],
   "source": [
    "# make tune data which is 1/10 of the train data\n",
    "X_tune_train = X_train[:len(X_train)//10]\n",
    "y_tune_train = y_train[:len(y_train)//10]\n",
    "X_tune_val = X_val[:len(X_val)//10]\n",
    "y_tune_val = y_val[:len(y_val)//10]\n",
    "\n",
    "# check to make sure our tuning datasets are indeed 1/10 of our previous training sets\n",
    "print(f'{X_tune_train.shape}, {X_train.shape}\\n{y_tune_train.shape}, {y_train.shape}')\n",
    "print(f'{X_tune_val.shape}, {X_val.shape}\\n{y_tune_val.shape}, {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1f313",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea6a5f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.582209</td>\n",
       "      <td>2.582209</td>\n",
       "      <td>0.162645</td>\n",
       "      <td>0.272276</td>\n",
       "      <td>2.562769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.568814</td>\n",
       "      <td>2.568814</td>\n",
       "      <td>0.172051</td>\n",
       "      <td>0.281843</td>\n",
       "      <td>2.642525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.558297</td>\n",
       "      <td>2.558297</td>\n",
       "      <td>0.178843</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>3.085897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.555101</td>\n",
       "      <td>2.555101</td>\n",
       "      <td>0.180604</td>\n",
       "      <td>0.288999</td>\n",
       "      <td>3.324303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.552860</td>\n",
       "      <td>2.552860</td>\n",
       "      <td>0.181744</td>\n",
       "      <td>0.290177</td>\n",
       "      <td>3.737888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2.551130</td>\n",
       "      <td>2.551130</td>\n",
       "      <td>0.182773</td>\n",
       "      <td>0.290471</td>\n",
       "      <td>3.885266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.551694</td>\n",
       "      <td>2.551694</td>\n",
       "      <td>0.182401</td>\n",
       "      <td>0.290341</td>\n",
       "      <td>4.080793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.550523</td>\n",
       "      <td>2.550523</td>\n",
       "      <td>0.183231</td>\n",
       "      <td>0.291159</td>\n",
       "      <td>4.518644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2.549250</td>\n",
       "      <td>2.549250</td>\n",
       "      <td>0.184042</td>\n",
       "      <td>0.292293</td>\n",
       "      <td>4.649842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2.549190</td>\n",
       "      <td>2.549190</td>\n",
       "      <td>0.184153</td>\n",
       "      <td>0.292381</td>\n",
       "      <td>4.871429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2.548096</td>\n",
       "      <td>2.548096</td>\n",
       "      <td>0.184766</td>\n",
       "      <td>0.293234</td>\n",
       "      <td>5.407768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.548965</td>\n",
       "      <td>2.548965</td>\n",
       "      <td>0.184188</td>\n",
       "      <td>0.292533</td>\n",
       "      <td>5.800142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2.548219</td>\n",
       "      <td>2.548219</td>\n",
       "      <td>0.184763</td>\n",
       "      <td>0.292646</td>\n",
       "      <td>6.147734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.547266</td>\n",
       "      <td>2.547266</td>\n",
       "      <td>0.185279</td>\n",
       "      <td>0.293184</td>\n",
       "      <td>6.195956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2.546875</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.293032</td>\n",
       "      <td>8.717478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2.546196</td>\n",
       "      <td>2.546196</td>\n",
       "      <td>0.185956</td>\n",
       "      <td>0.293190</td>\n",
       "      <td>7.326823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2.545753</td>\n",
       "      <td>2.545753</td>\n",
       "      <td>0.186152</td>\n",
       "      <td>0.293549</td>\n",
       "      <td>7.331218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2.544758</td>\n",
       "      <td>2.544758</td>\n",
       "      <td>0.186767</td>\n",
       "      <td>0.293626</td>\n",
       "      <td>7.046473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.543839</td>\n",
       "      <td>2.543839</td>\n",
       "      <td>0.187251</td>\n",
       "      <td>0.293796</td>\n",
       "      <td>7.217258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2.543672</td>\n",
       "      <td>2.543672</td>\n",
       "      <td>0.187274</td>\n",
       "      <td>0.293749</td>\n",
       "      <td>7.627687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "10            2.582209             2.582209      0.162645        0.272276   \n",
       "20            2.568814             2.568814      0.172051        0.281843   \n",
       "30            2.558297             2.558297      0.178843        0.286912   \n",
       "40            2.555101             2.555101      0.180604        0.288999   \n",
       "50            2.552860             2.552860      0.181744        0.290177   \n",
       "60            2.551130             2.551130      0.182773        0.290471   \n",
       "70            2.551694             2.551694      0.182401        0.290341   \n",
       "80            2.550523             2.550523      0.183231        0.291159   \n",
       "90            2.549250             2.549250      0.184042        0.292293   \n",
       "100           2.549190             2.549190      0.184153        0.292381   \n",
       "110           2.548096             2.548096      0.184766        0.293234   \n",
       "120           2.548965             2.548965      0.184188        0.292533   \n",
       "130           2.548219             2.548219      0.184763        0.292646   \n",
       "140           2.547266             2.547266      0.185279        0.293184   \n",
       "150           2.546875             2.546875      0.185543        0.293032   \n",
       "160           2.546196             2.546196      0.185956        0.293190   \n",
       "170           2.545753             2.545753      0.186152        0.293549   \n",
       "180           2.544758             2.544758      0.186767        0.293626   \n",
       "190           2.543839             2.543839      0.187251        0.293796   \n",
       "200           2.543672             2.543672      0.187274        0.293749   \n",
       "\n",
       "         Time  \n",
       "10   2.562769  \n",
       "20   2.642525  \n",
       "30   3.085897  \n",
       "40   3.324303  \n",
       "50   3.737888  \n",
       "60   3.885266  \n",
       "70   4.080793  \n",
       "80   4.518644  \n",
       "90   4.649842  \n",
       "100  4.871429  \n",
       "110  5.407768  \n",
       "120  5.800142  \n",
       "130  6.147734  \n",
       "140  6.195956  \n",
       "150  8.717478  \n",
       "160  7.326823  \n",
       "170  7.331218  \n",
       "180  7.046473  \n",
       "190  7.217258  \n",
       "200  7.627687  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first hyperparameter to optimize in a random forest is the number of trees\n",
    "# obviously more trees will only increase the accuracy so we will instead examine when the accuracy approaches\n",
    "# an asymptote - we will also learn how many estimators we should use for further tuning of hyperparameters, as\n",
    "# we want enough so that the accuracy is similar to the best accuracy but also not too many for time constraints\n",
    "\n",
    "estimators = range(10, 201, 10)\n",
    "train_acc_rf = []\n",
    "val_acc_rf = []\n",
    "train_r2_rf = []\n",
    "val_r2_rf = []\n",
    "seconds = []\n",
    "\n",
    "# we will use the previously determined max_depth hyperparameter\n",
    "for estimator in estimators:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                        StandardScaler(),\n",
    "                        SimpleImputer(strategy = 'mean'),\n",
    "                        RandomForestRegressor(random_state = 42,\n",
    "                                              n_jobs = -1,\n",
    "                                              max_depth = 12,\n",
    "                                              n_estimators = estimator))\n",
    "    \n",
    "    model.fit(X_tune_train, y_tune_train)\n",
    "    \n",
    "    train_acc_rf.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_rf.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    train_r2_rf.append(model.score(X_train, y_train))\n",
    "    val_r2_rf.append(model.score(X_val, y_val))\n",
    "    seconds.append(time.time() - start)\n",
    "    \n",
    "pd.DataFrame(list(zip(train_acc_rf, val_acc_rf, train_r2_rf, val_r2_rf, seconds)), index = estimators,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb4eb3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>2.615812</td>\n",
       "      <td>2.615812</td>\n",
       "      <td>0.170885</td>\n",
       "      <td>0.310412</td>\n",
       "      <td>2.443685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>2.609338</td>\n",
       "      <td>2.609338</td>\n",
       "      <td>0.170314</td>\n",
       "      <td>0.306717</td>\n",
       "      <td>2.332825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>2.591294</td>\n",
       "      <td>2.591294</td>\n",
       "      <td>0.178513</td>\n",
       "      <td>0.311663</td>\n",
       "      <td>2.335235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>2.580971</td>\n",
       "      <td>2.580971</td>\n",
       "      <td>0.183085</td>\n",
       "      <td>0.313654</td>\n",
       "      <td>2.477202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>2.587098</td>\n",
       "      <td>2.587098</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.304289</td>\n",
       "      <td>2.497575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>2.570444</td>\n",
       "      <td>2.570444</td>\n",
       "      <td>0.183020</td>\n",
       "      <td>0.306227</td>\n",
       "      <td>2.570322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>2.572279</td>\n",
       "      <td>2.572279</td>\n",
       "      <td>0.179418</td>\n",
       "      <td>0.304073</td>\n",
       "      <td>2.622693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>2.556248</td>\n",
       "      <td>2.556248</td>\n",
       "      <td>0.187857</td>\n",
       "      <td>0.306602</td>\n",
       "      <td>2.522264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>2.565814</td>\n",
       "      <td>2.565814</td>\n",
       "      <td>0.182614</td>\n",
       "      <td>0.301132</td>\n",
       "      <td>2.790190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>2.565180</td>\n",
       "      <td>2.565180</td>\n",
       "      <td>0.181938</td>\n",
       "      <td>0.302706</td>\n",
       "      <td>3.524909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>2.573845</td>\n",
       "      <td>2.573845</td>\n",
       "      <td>0.174243</td>\n",
       "      <td>0.293223</td>\n",
       "      <td>3.099631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>2.567845</td>\n",
       "      <td>2.567845</td>\n",
       "      <td>0.177097</td>\n",
       "      <td>0.289759</td>\n",
       "      <td>3.510155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>2.564178</td>\n",
       "      <td>2.564178</td>\n",
       "      <td>0.179464</td>\n",
       "      <td>0.297196</td>\n",
       "      <td>3.464510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>2.560909</td>\n",
       "      <td>2.560909</td>\n",
       "      <td>0.181778</td>\n",
       "      <td>0.291799</td>\n",
       "      <td>3.637746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>2.567268</td>\n",
       "      <td>2.567268</td>\n",
       "      <td>0.176251</td>\n",
       "      <td>0.290948</td>\n",
       "      <td>3.099570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>2.550933</td>\n",
       "      <td>2.550933</td>\n",
       "      <td>0.185945</td>\n",
       "      <td>0.294909</td>\n",
       "      <td>3.161065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>2.563513</td>\n",
       "      <td>2.563513</td>\n",
       "      <td>0.177915</td>\n",
       "      <td>0.292637</td>\n",
       "      <td>4.092125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>2.552285</td>\n",
       "      <td>2.552285</td>\n",
       "      <td>0.182985</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>3.446269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>2.555581</td>\n",
       "      <td>2.555581</td>\n",
       "      <td>0.180227</td>\n",
       "      <td>0.289304</td>\n",
       "      <td>3.260305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.05           2.615812             2.615812      0.170885        0.310412   \n",
       "0.10           2.609338             2.609338      0.170314        0.306717   \n",
       "0.15           2.591294             2.591294      0.178513        0.311663   \n",
       "0.20           2.580971             2.580971      0.183085        0.313654   \n",
       "0.25           2.587098             2.587098      0.175954        0.304289   \n",
       "0.30           2.570444             2.570444      0.183020        0.306227   \n",
       "0.35           2.572279             2.572279      0.179418        0.304073   \n",
       "0.40           2.556248             2.556248      0.187857        0.306602   \n",
       "0.45           2.565814             2.565814      0.182614        0.301132   \n",
       "0.50           2.565180             2.565180      0.181938        0.302706   \n",
       "0.55           2.573845             2.573845      0.174243        0.293223   \n",
       "0.60           2.567845             2.567845      0.177097        0.289759   \n",
       "0.65           2.564178             2.564178      0.179464        0.297196   \n",
       "0.70           2.560909             2.560909      0.181778        0.291799   \n",
       "0.75           2.567268             2.567268      0.176251        0.290948   \n",
       "0.80           2.550933             2.550933      0.185945        0.294909   \n",
       "0.85           2.563513             2.563513      0.177915        0.292637   \n",
       "0.90           2.552285             2.552285      0.182985        0.294327   \n",
       "0.95           2.555581             2.555581      0.180227        0.289304   \n",
       "\n",
       "          Time  \n",
       "0.05  2.443685  \n",
       "0.10  2.332825  \n",
       "0.15  2.335235  \n",
       "0.20  2.477202  \n",
       "0.25  2.497575  \n",
       "0.30  2.570322  \n",
       "0.35  2.622693  \n",
       "0.40  2.522264  \n",
       "0.45  2.790190  \n",
       "0.50  3.524909  \n",
       "0.55  3.099631  \n",
       "0.60  3.510155  \n",
       "0.65  3.464510  \n",
       "0.70  3.637746  \n",
       "0.75  3.099570  \n",
       "0.80  3.161065  \n",
       "0.85  4.092125  \n",
       "0.90  3.446269  \n",
       "0.95  3.260305  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine what percentage of the data to sample for each decision tree\n",
    "samples = np.arange(0.05, 1, 0.05)\n",
    "train_acc_rf = []\n",
    "val_acc_rf = []\n",
    "train_r2_rf = []\n",
    "val_r2_rf = []\n",
    "seconds = []\n",
    "\n",
    "for sample in samples:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                        StandardScaler(),\n",
    "                        SimpleImputer(strategy = 'mean'),\n",
    "                        RandomForestRegressor(random_state = 42,\n",
    "                                              n_jobs = -1,\n",
    "                                              max_depth = 12,\n",
    "                                              n_estimators = 30,\n",
    "                                              max_samples = sample))\n",
    "    model.fit(X_tune_train, y_tune_train)\n",
    "    \n",
    "    train_acc_rf.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_rf.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    train_r2_rf.append(model.score(X_train, y_train))\n",
    "    val_r2_rf.append(model.score(X_val, y_val))\n",
    "    seconds.append(time.time() - start)\n",
    "    \n",
    "pd.DataFrame(list(zip(train_acc_rf, val_acc_rf, train_r2_rf, val_r2_rf, seconds)), index = samples,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "802630a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                        StandardScaler(),\n",
    "                        SimpleImputer(strategy = 'mean'),\n",
    "                        RandomForestRegressor(random_state = 42,\n",
    "                                              n_jobs = -1,\n",
    "                                              max_depth = 12,\n",
    "                                              n_estimators = 180,\n",
    "                                              max_samples = 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c05414f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 1.6356197620259867\n",
      "Validation MAE: 2.173502003913109\n",
      "Validation R2: 0.48461457999081914\n",
      "CPU times: user 5min 23s, sys: 2.75 s, total: 5min 26s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_rf.fit(X_train, y_train)\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_rf.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_rf.predict(X_val)))\n",
    "print('Validation R2:', model_rf.score(X_val,y_val))\n",
    "\n",
    "# interestingly, our random forest model is not significantly better than our decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8f3d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the top 3 importances are the same as the decision tree\n",
    "importances = model_rf.named_steps['randomforestregressor'].feature_importances_\n",
    "features = model_rf.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp_rf = pd.Series(importances, index = features,\n",
    "                       name = 'forest').abs().sort_values(ascending=False)\n",
    "feat_imp_rf.head(10).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c7c15",
   "metadata": {},
   "source": [
    "# Boost Model\n",
    "After testing out decision trees, random forests, boosted random forests, linear regression, and ridge regression, I found that the boosted model gave the lowest MAE and highest R^2 by a significant amount. Given this information, I decided to retrain every hyperparameter instead of assuming that overlapping parameters with other models like max_depth would have the same optimized value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "274cf8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.022567</td>\n",
       "      <td>2.411168</td>\n",
       "      <td>0.506516</td>\n",
       "      <td>0.409007</td>\n",
       "      <td>6.229483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.067171</td>\n",
       "      <td>1.462983</td>\n",
       "      <td>0.851477</td>\n",
       "      <td>0.775270</td>\n",
       "      <td>13.775368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.559976</td>\n",
       "      <td>0.981228</td>\n",
       "      <td>0.958987</td>\n",
       "      <td>0.888616</td>\n",
       "      <td>24.352070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.248948</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.991527</td>\n",
       "      <td>0.896002</td>\n",
       "      <td>36.619136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.063389</td>\n",
       "      <td>1.106082</td>\n",
       "      <td>0.999310</td>\n",
       "      <td>0.849355</td>\n",
       "      <td>49.131865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008349</td>\n",
       "      <td>1.224174</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.813134</td>\n",
       "      <td>68.569854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000465</td>\n",
       "      <td>1.286523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792358</td>\n",
       "      <td>86.724962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "1            2.022567             2.411168      0.506516        0.409007   \n",
       "4            1.067171             1.462983      0.851477        0.775270   \n",
       "7            0.559976             0.981228      0.958987        0.888616   \n",
       "10           0.248948             0.922667      0.991527        0.896002   \n",
       "13           0.063389             1.106082      0.999310        0.849355   \n",
       "16           0.008349             1.224174      0.999985        0.813134   \n",
       "19           0.000465             1.286523      1.000000        0.792358   \n",
       "\n",
       "         Time  \n",
       "1    6.229483  \n",
       "4   13.775368  \n",
       "7   24.352070  \n",
       "10  36.619136  \n",
       "13  49.131865  \n",
       "16  68.569854  \n",
       "19  86.724962  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the optimal depth\n",
    "depths = range(1, 20, 3)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "times = []\n",
    "\n",
    "for depth in depths:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = depth))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, times)), index = depths,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f08887c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.559976</td>\n",
       "      <td>0.981228</td>\n",
       "      <td>0.958987</td>\n",
       "      <td>0.888616</td>\n",
       "      <td>22.302084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.451831</td>\n",
       "      <td>0.936901</td>\n",
       "      <td>0.973493</td>\n",
       "      <td>0.897158</td>\n",
       "      <td>24.917931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.336773</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.985041</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>31.290033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.248948</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.991527</td>\n",
       "      <td>0.896002</td>\n",
       "      <td>38.799618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.172464</td>\n",
       "      <td>0.971304</td>\n",
       "      <td>0.995641</td>\n",
       "      <td>0.882165</td>\n",
       "      <td>40.825115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.102199</td>\n",
       "      <td>0.985658</td>\n",
       "      <td>0.998345</td>\n",
       "      <td>0.881312</td>\n",
       "      <td>42.636395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "7            0.559976             0.981228      0.958987        0.888616   \n",
       "8            0.451831             0.936901      0.973493        0.897158   \n",
       "9            0.336773             0.906595      0.985041        0.901681   \n",
       "10           0.248948             0.922667      0.991527        0.896002   \n",
       "11           0.172464             0.971304      0.995641        0.882165   \n",
       "12           0.102199             0.985658      0.998345        0.881312   \n",
       "\n",
       "         Time  \n",
       "7   22.302084  \n",
       "8   24.917931  \n",
       "9   31.290033  \n",
       "10  38.799618  \n",
       "11  40.825115  \n",
       "12  42.636395  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still finding the optimal depth\n",
    "depths = range(7, 13)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "times = []\n",
    "\n",
    "for depth in depths:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = depth))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, times)), index = depths,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])\n",
    "# optimal depth is 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a75d7b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.458921</td>\n",
       "      <td>2.008019</td>\n",
       "      <td>0.718010</td>\n",
       "      <td>0.554908</td>\n",
       "      <td>6.007543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.550692</td>\n",
       "      <td>1.073716</td>\n",
       "      <td>0.959439</td>\n",
       "      <td>0.865710</td>\n",
       "      <td>17.525789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.336773</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.985041</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>32.649476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.251377</td>\n",
       "      <td>0.861896</td>\n",
       "      <td>0.991494</td>\n",
       "      <td>0.910202</td>\n",
       "      <td>42.637807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.200457</td>\n",
       "      <td>0.846407</td>\n",
       "      <td>0.994434</td>\n",
       "      <td>0.913051</td>\n",
       "      <td>54.431116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.167735</td>\n",
       "      <td>0.841513</td>\n",
       "      <td>0.995969</td>\n",
       "      <td>0.913980</td>\n",
       "      <td>67.055414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "10            1.458921             2.008019      0.718010        0.554908   \n",
       "50            0.550692             1.073716      0.959439        0.865710   \n",
       "100           0.336773             0.906595      0.985041        0.901681   \n",
       "150           0.251377             0.861896      0.991494        0.910202   \n",
       "200           0.200457             0.846407      0.994434        0.913051   \n",
       "250           0.167735             0.841513      0.995969        0.913980   \n",
       "\n",
       "          Time  \n",
       "10    6.007543  \n",
       "50   17.525789  \n",
       "100  32.649476  \n",
       "150  42.637807  \n",
       "200  54.431116  \n",
       "250  67.055414  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing how number of trees affects the model accuracy\n",
    "estimators = [10] + [i for i in range(50, 300, 50)]\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "times = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = estimator))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, times)), index = estimators,\n",
    "            columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', 'Time'])\n",
    "# 100 trees should be enough for optimizing hyperparameters, but use >200 for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "863704da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>4.305632</td>\n",
       "      <td>4.873540</td>\n",
       "      <td>-0.921013</td>\n",
       "      <td>-0.995827</td>\n",
       "      <td>27.518515</td>\n",
       "      <td>29.380582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>0.876257</td>\n",
       "      <td>1.379132</td>\n",
       "      <td>0.895031</td>\n",
       "      <td>0.784209</td>\n",
       "      <td>27.018967</td>\n",
       "      <td>28.907143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>0.600575</td>\n",
       "      <td>1.120328</td>\n",
       "      <td>0.950245</td>\n",
       "      <td>0.852466</td>\n",
       "      <td>27.263737</td>\n",
       "      <td>29.134317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>0.471246</td>\n",
       "      <td>1.005573</td>\n",
       "      <td>0.969751</td>\n",
       "      <td>0.879152</td>\n",
       "      <td>28.410104</td>\n",
       "      <td>30.141910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.398929</td>\n",
       "      <td>0.949165</td>\n",
       "      <td>0.978735</td>\n",
       "      <td>0.892096</td>\n",
       "      <td>27.585426</td>\n",
       "      <td>29.463331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.348024</td>\n",
       "      <td>0.893254</td>\n",
       "      <td>0.984035</td>\n",
       "      <td>0.902871</td>\n",
       "      <td>27.454771</td>\n",
       "      <td>29.316017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.01           4.305632             4.873540     -0.921013       -0.995827   \n",
       "0.06           0.876257             1.379132      0.895031        0.784209   \n",
       "0.11           0.600575             1.120328      0.950245        0.852466   \n",
       "0.16           0.471246             1.005573      0.969751        0.879152   \n",
       "0.21           0.398929             0.949165      0.978735        0.892096   \n",
       "0.26           0.348024             0.893254      0.984035        0.902871   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.01   27.518515   29.380582  \n",
       "0.06   27.018967   28.907143  \n",
       "0.11   27.263737   29.134317  \n",
       "0.16   28.410104   30.141910  \n",
       "0.21   27.585426   29.463331  \n",
       "0.26   27.454771   29.316017  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing learning rate\n",
    "learns = np.arange(0.01, 0.31, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for learn in learns:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = learn))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = learns,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0fbe45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.374583</td>\n",
       "      <td>0.930133</td>\n",
       "      <td>0.981489</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>27.072686</td>\n",
       "      <td>29.032215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.336773</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.985041</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>29.615869</td>\n",
       "      <td>31.473083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>0.325494</td>\n",
       "      <td>0.909625</td>\n",
       "      <td>0.986098</td>\n",
       "      <td>0.900672</td>\n",
       "      <td>27.444335</td>\n",
       "      <td>29.330502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>0.335998</td>\n",
       "      <td>0.972642</td>\n",
       "      <td>0.985106</td>\n",
       "      <td>0.889159</td>\n",
       "      <td>26.720082</td>\n",
       "      <td>28.597790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>0.328752</td>\n",
       "      <td>0.993242</td>\n",
       "      <td>0.985602</td>\n",
       "      <td>0.884518</td>\n",
       "      <td>27.156677</td>\n",
       "      <td>29.134295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.324245</td>\n",
       "      <td>0.989111</td>\n",
       "      <td>0.985841</td>\n",
       "      <td>0.886558</td>\n",
       "      <td>28.117626</td>\n",
       "      <td>30.031494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.25           0.374583             0.930133      0.981489        0.894100   \n",
       "0.30           0.336773             0.906595      0.985041        0.901681   \n",
       "0.35           0.325494             0.909625      0.986098        0.900672   \n",
       "0.40           0.335998             0.972642      0.985106        0.889159   \n",
       "0.45           0.328752             0.993242      0.985602        0.884518   \n",
       "0.50           0.324245             0.989111      0.985841        0.886558   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.25   27.072686   29.032215  \n",
       "0.30   29.615869   31.473083  \n",
       "0.35   27.444335   29.330502  \n",
       "0.40   26.720082   28.597790  \n",
       "0.45   27.156677   29.134295  \n",
       "0.50   28.117626   30.031494  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to expand learning rate parameter as accuracy kept increasing when learning rate hit 0.26\n",
    "learns = np.arange(0.25, 0.51, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for learn in learns:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = learn))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = learns,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d21ca7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.374583</td>\n",
       "      <td>0.930133</td>\n",
       "      <td>0.981489</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>28.336491</td>\n",
       "      <td>30.295004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.348024</td>\n",
       "      <td>0.893254</td>\n",
       "      <td>0.984035</td>\n",
       "      <td>0.902871</td>\n",
       "      <td>28.561859</td>\n",
       "      <td>30.672797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.27</th>\n",
       "      <td>0.360092</td>\n",
       "      <td>0.926709</td>\n",
       "      <td>0.982878</td>\n",
       "      <td>0.897102</td>\n",
       "      <td>28.271320</td>\n",
       "      <td>30.018318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.28</th>\n",
       "      <td>0.351093</td>\n",
       "      <td>0.928276</td>\n",
       "      <td>0.983790</td>\n",
       "      <td>0.897119</td>\n",
       "      <td>26.917110</td>\n",
       "      <td>28.807216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.29</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>27.685126</td>\n",
       "      <td>29.574629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>0.336773</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.985041</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>26.960374</td>\n",
       "      <td>28.797447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>0.326774</td>\n",
       "      <td>0.875840</td>\n",
       "      <td>0.985994</td>\n",
       "      <td>0.906885</td>\n",
       "      <td>27.769066</td>\n",
       "      <td>29.674996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.32</th>\n",
       "      <td>0.329934</td>\n",
       "      <td>0.913480</td>\n",
       "      <td>0.985770</td>\n",
       "      <td>0.899803</td>\n",
       "      <td>27.565171</td>\n",
       "      <td>29.466691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.33</th>\n",
       "      <td>0.351560</td>\n",
       "      <td>0.944354</td>\n",
       "      <td>0.983843</td>\n",
       "      <td>0.893561</td>\n",
       "      <td>27.175205</td>\n",
       "      <td>29.062419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.34</th>\n",
       "      <td>0.331155</td>\n",
       "      <td>0.923316</td>\n",
       "      <td>0.985633</td>\n",
       "      <td>0.897106</td>\n",
       "      <td>27.457212</td>\n",
       "      <td>29.271358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.25           0.374583             0.930133      0.981489        0.894100   \n",
       "0.26           0.348024             0.893254      0.984035        0.902871   \n",
       "0.27           0.360092             0.926709      0.982878        0.897102   \n",
       "0.28           0.351093             0.928276      0.983790        0.897119   \n",
       "0.29           0.344965             0.899407      0.984345        0.902267   \n",
       "0.30           0.336773             0.906595      0.985041        0.901681   \n",
       "0.31           0.326774             0.875840      0.985994        0.906885   \n",
       "0.32           0.329934             0.913480      0.985770        0.899803   \n",
       "0.33           0.351560             0.944354      0.983843        0.893561   \n",
       "0.34           0.331155             0.923316      0.985633        0.897106   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.25   28.336491   30.295004  \n",
       "0.26   28.561859   30.672797  \n",
       "0.27   28.271320   30.018318  \n",
       "0.28   26.917110   28.807216  \n",
       "0.29   27.685126   29.574629  \n",
       "0.30   26.960374   28.797447  \n",
       "0.31   27.769066   29.674996  \n",
       "0.32   27.565171   29.466691  \n",
       "0.33   27.175205   29.062419  \n",
       "0.34   27.457212   29.271358  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the true optimal learning rate\n",
    "learns = np.arange(0.25, 0.35, 0.01)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for learn in learns:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = learn))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = learns,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "82945968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>2.265132</td>\n",
       "      <td>2.658796</td>\n",
       "      <td>0.405386</td>\n",
       "      <td>0.316841</td>\n",
       "      <td>3.407428</td>\n",
       "      <td>5.271665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>1.730661</td>\n",
       "      <td>2.401781</td>\n",
       "      <td>0.630588</td>\n",
       "      <td>0.409034</td>\n",
       "      <td>5.026938</td>\n",
       "      <td>6.974753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>1.563487</td>\n",
       "      <td>2.359201</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.422647</td>\n",
       "      <td>5.689579</td>\n",
       "      <td>7.521530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>1.478895</td>\n",
       "      <td>2.300724</td>\n",
       "      <td>0.722904</td>\n",
       "      <td>0.452204</td>\n",
       "      <td>6.607101</td>\n",
       "      <td>8.415193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>1.376448</td>\n",
       "      <td>2.178777</td>\n",
       "      <td>0.759138</td>\n",
       "      <td>0.506195</td>\n",
       "      <td>7.703148</td>\n",
       "      <td>9.546548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>1.341305</td>\n",
       "      <td>2.136572</td>\n",
       "      <td>0.769855</td>\n",
       "      <td>0.524740</td>\n",
       "      <td>8.333301</td>\n",
       "      <td>10.158824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>1.110421</td>\n",
       "      <td>1.842890</td>\n",
       "      <td>0.839383</td>\n",
       "      <td>0.636588</td>\n",
       "      <td>9.379836</td>\n",
       "      <td>11.208366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>1.034855</td>\n",
       "      <td>1.755981</td>\n",
       "      <td>0.860039</td>\n",
       "      <td>0.669203</td>\n",
       "      <td>10.413425</td>\n",
       "      <td>12.166705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>1.006738</td>\n",
       "      <td>1.736587</td>\n",
       "      <td>0.867954</td>\n",
       "      <td>0.678911</td>\n",
       "      <td>11.303019</td>\n",
       "      <td>13.112994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>0.864011</td>\n",
       "      <td>1.512336</td>\n",
       "      <td>0.902516</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>11.705318</td>\n",
       "      <td>13.553090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51</th>\n",
       "      <td>0.791057</td>\n",
       "      <td>1.401077</td>\n",
       "      <td>0.918283</td>\n",
       "      <td>0.786031</td>\n",
       "      <td>12.564776</td>\n",
       "      <td>14.342873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56</th>\n",
       "      <td>0.765814</td>\n",
       "      <td>1.361709</td>\n",
       "      <td>0.921681</td>\n",
       "      <td>0.791194</td>\n",
       "      <td>13.298953</td>\n",
       "      <td>15.095393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61</th>\n",
       "      <td>0.670059</td>\n",
       "      <td>1.197534</td>\n",
       "      <td>0.940722</td>\n",
       "      <td>0.837080</td>\n",
       "      <td>14.946395</td>\n",
       "      <td>16.752769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.66</th>\n",
       "      <td>0.642752</td>\n",
       "      <td>1.159426</td>\n",
       "      <td>0.945281</td>\n",
       "      <td>0.846236</td>\n",
       "      <td>16.132803</td>\n",
       "      <td>18.063057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71</th>\n",
       "      <td>0.630533</td>\n",
       "      <td>1.134668</td>\n",
       "      <td>0.946919</td>\n",
       "      <td>0.849843</td>\n",
       "      <td>17.777698</td>\n",
       "      <td>19.548663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.76</th>\n",
       "      <td>0.604242</td>\n",
       "      <td>1.092757</td>\n",
       "      <td>0.951817</td>\n",
       "      <td>0.862488</td>\n",
       "      <td>19.227515</td>\n",
       "      <td>21.055678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.81</th>\n",
       "      <td>0.602777</td>\n",
       "      <td>1.083068</td>\n",
       "      <td>0.952012</td>\n",
       "      <td>0.863989</td>\n",
       "      <td>18.165578</td>\n",
       "      <td>20.007493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.86</th>\n",
       "      <td>0.584910</td>\n",
       "      <td>1.031749</td>\n",
       "      <td>0.955252</td>\n",
       "      <td>0.877805</td>\n",
       "      <td>18.121230</td>\n",
       "      <td>19.921756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.91</th>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.998495</td>\n",
       "      <td>0.956680</td>\n",
       "      <td>0.886138</td>\n",
       "      <td>18.989281</td>\n",
       "      <td>20.813402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.575918</td>\n",
       "      <td>0.989802</td>\n",
       "      <td>0.956844</td>\n",
       "      <td>0.889016</td>\n",
       "      <td>19.576406</td>\n",
       "      <td>21.401110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.01           2.265132             2.658796      0.405386        0.316841   \n",
       "0.06           1.730661             2.401781      0.630588        0.409034   \n",
       "0.11           1.563487             2.359201      0.692600        0.422647   \n",
       "0.16           1.478895             2.300724      0.722904        0.452204   \n",
       "0.21           1.376448             2.178777      0.759138        0.506195   \n",
       "0.26           1.341305             2.136572      0.769855        0.524740   \n",
       "0.31           1.110421             1.842890      0.839383        0.636588   \n",
       "0.36           1.034855             1.755981      0.860039        0.669203   \n",
       "0.41           1.006738             1.736587      0.867954        0.678911   \n",
       "0.46           0.864011             1.512336      0.902516        0.750600   \n",
       "0.51           0.791057             1.401077      0.918283        0.786031   \n",
       "0.56           0.765814             1.361709      0.921681        0.791194   \n",
       "0.61           0.670059             1.197534      0.940722        0.837080   \n",
       "0.66           0.642752             1.159426      0.945281        0.846236   \n",
       "0.71           0.630533             1.134668      0.946919        0.849843   \n",
       "0.76           0.604242             1.092757      0.951817        0.862488   \n",
       "0.81           0.602777             1.083068      0.952012        0.863989   \n",
       "0.86           0.584910             1.031749      0.955252        0.877805   \n",
       "0.91           0.575419             0.998495      0.956680        0.886138   \n",
       "0.96           0.575918             0.989802      0.956844        0.889016   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.01    3.407428    5.271665  \n",
       "0.06    5.026938    6.974753  \n",
       "0.11    5.689579    7.521530  \n",
       "0.16    6.607101    8.415193  \n",
       "0.21    7.703148    9.546548  \n",
       "0.26    8.333301   10.158824  \n",
       "0.31    9.379836   11.208366  \n",
       "0.36   10.413425   12.166705  \n",
       "0.41   11.303019   13.112994  \n",
       "0.46   11.705318   13.553090  \n",
       "0.51   12.564776   14.342873  \n",
       "0.56   13.298953   15.095393  \n",
       "0.61   14.946395   16.752769  \n",
       "0.66   16.132803   18.063057  \n",
       "0.71   17.777698   19.548663  \n",
       "0.76   19.227515   21.055678  \n",
       "0.81   18.165578   20.007493  \n",
       "0.86   18.121230   19.921756  \n",
       "0.91   18.989281   20.813402  \n",
       "0.96   19.576406   21.401110  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing colsample_by tree hyperparameter, i.e. the subsample ratio for the columns when constructing\n",
    "# the dataset for each tree\n",
    "samples = np.arange(0.01, 1.00, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for sample in samples:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = sample))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = samples,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "92b3b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>1.798353</td>\n",
       "      <td>2.429155</td>\n",
       "      <td>0.602226</td>\n",
       "      <td>0.400073</td>\n",
       "      <td>4.169355</td>\n",
       "      <td>6.021555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>1.280633</td>\n",
       "      <td>2.322733</td>\n",
       "      <td>0.788227</td>\n",
       "      <td>0.448029</td>\n",
       "      <td>6.009362</td>\n",
       "      <td>7.882564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>1.004720</td>\n",
       "      <td>2.069119</td>\n",
       "      <td>0.867691</td>\n",
       "      <td>0.557191</td>\n",
       "      <td>7.178469</td>\n",
       "      <td>9.024422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>0.798174</td>\n",
       "      <td>1.814976</td>\n",
       "      <td>0.915653</td>\n",
       "      <td>0.652846</td>\n",
       "      <td>8.451295</td>\n",
       "      <td>10.258226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.643834</td>\n",
       "      <td>1.564899</td>\n",
       "      <td>0.944326</td>\n",
       "      <td>0.733532</td>\n",
       "      <td>10.331754</td>\n",
       "      <td>12.192414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.549895</td>\n",
       "      <td>1.404284</td>\n",
       "      <td>0.960203</td>\n",
       "      <td>0.784755</td>\n",
       "      <td>10.826890</td>\n",
       "      <td>12.612990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>0.492136</td>\n",
       "      <td>1.280645</td>\n",
       "      <td>0.968289</td>\n",
       "      <td>0.814784</td>\n",
       "      <td>12.176496</td>\n",
       "      <td>14.042352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>0.472485</td>\n",
       "      <td>1.264556</td>\n",
       "      <td>0.970903</td>\n",
       "      <td>0.820409</td>\n",
       "      <td>13.426852</td>\n",
       "      <td>15.244076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>0.451787</td>\n",
       "      <td>1.185316</td>\n",
       "      <td>0.973174</td>\n",
       "      <td>0.840025</td>\n",
       "      <td>14.882225</td>\n",
       "      <td>16.617881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>0.445414</td>\n",
       "      <td>1.174575</td>\n",
       "      <td>0.974025</td>\n",
       "      <td>0.842993</td>\n",
       "      <td>14.654241</td>\n",
       "      <td>16.416245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51</th>\n",
       "      <td>0.400830</td>\n",
       "      <td>1.079140</td>\n",
       "      <td>0.978969</td>\n",
       "      <td>0.865219</td>\n",
       "      <td>15.587495</td>\n",
       "      <td>17.352338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56</th>\n",
       "      <td>0.395668</td>\n",
       "      <td>1.099082</td>\n",
       "      <td>0.979558</td>\n",
       "      <td>0.859394</td>\n",
       "      <td>16.798884</td>\n",
       "      <td>18.545358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61</th>\n",
       "      <td>0.387309</td>\n",
       "      <td>1.041369</td>\n",
       "      <td>0.980409</td>\n",
       "      <td>0.874663</td>\n",
       "      <td>17.834575</td>\n",
       "      <td>19.609918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.66</th>\n",
       "      <td>0.370032</td>\n",
       "      <td>1.016352</td>\n",
       "      <td>0.982228</td>\n",
       "      <td>0.876135</td>\n",
       "      <td>18.892614</td>\n",
       "      <td>20.672909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71</th>\n",
       "      <td>0.384226</td>\n",
       "      <td>1.025160</td>\n",
       "      <td>0.980625</td>\n",
       "      <td>0.876351</td>\n",
       "      <td>20.339566</td>\n",
       "      <td>22.119980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.76</th>\n",
       "      <td>0.375324</td>\n",
       "      <td>1.012480</td>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>21.196828</td>\n",
       "      <td>22.939906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.81</th>\n",
       "      <td>0.382465</td>\n",
       "      <td>1.022908</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.878211</td>\n",
       "      <td>22.736584</td>\n",
       "      <td>24.453088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.86</th>\n",
       "      <td>0.370286</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>0.982020</td>\n",
       "      <td>0.885094</td>\n",
       "      <td>23.228545</td>\n",
       "      <td>24.963649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.91</th>\n",
       "      <td>0.368111</td>\n",
       "      <td>0.970915</td>\n",
       "      <td>0.982332</td>\n",
       "      <td>0.888198</td>\n",
       "      <td>24.857606</td>\n",
       "      <td>26.672605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.353062</td>\n",
       "      <td>0.947312</td>\n",
       "      <td>0.983645</td>\n",
       "      <td>0.892316</td>\n",
       "      <td>27.781894</td>\n",
       "      <td>29.667274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.01           1.798353             2.429155      0.602226        0.400073   \n",
       "0.06           1.280633             2.322733      0.788227        0.448029   \n",
       "0.11           1.004720             2.069119      0.867691        0.557191   \n",
       "0.16           0.798174             1.814976      0.915653        0.652846   \n",
       "0.21           0.643834             1.564899      0.944326        0.733532   \n",
       "0.26           0.549895             1.404284      0.960203        0.784755   \n",
       "0.31           0.492136             1.280645      0.968289        0.814784   \n",
       "0.36           0.472485             1.264556      0.970903        0.820409   \n",
       "0.41           0.451787             1.185316      0.973174        0.840025   \n",
       "0.46           0.445414             1.174575      0.974025        0.842993   \n",
       "0.51           0.400830             1.079140      0.978969        0.865219   \n",
       "0.56           0.395668             1.099082      0.979558        0.859394   \n",
       "0.61           0.387309             1.041369      0.980409        0.874663   \n",
       "0.66           0.370032             1.016352      0.982228        0.876135   \n",
       "0.71           0.384226             1.025160      0.980625        0.876351   \n",
       "0.76           0.375324             1.012480      0.981605        0.879747   \n",
       "0.81           0.382465             1.022908      0.980805        0.878211   \n",
       "0.86           0.370286             0.983759      0.982020        0.885094   \n",
       "0.91           0.368111             0.970915      0.982332        0.888198   \n",
       "0.96           0.353062             0.947312      0.983645        0.892316   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.01    4.169355    6.021555  \n",
       "0.06    6.009362    7.882564  \n",
       "0.11    7.178469    9.024422  \n",
       "0.16    8.451295   10.258226  \n",
       "0.21   10.331754   12.192414  \n",
       "0.26   10.826890   12.612990  \n",
       "0.31   12.176496   14.042352  \n",
       "0.36   13.426852   15.244076  \n",
       "0.41   14.882225   16.617881  \n",
       "0.46   14.654241   16.416245  \n",
       "0.51   15.587495   17.352338  \n",
       "0.56   16.798884   18.545358  \n",
       "0.61   17.834575   19.609918  \n",
       "0.66   18.892614   20.672909  \n",
       "0.71   20.339566   22.119980  \n",
       "0.76   21.196828   22.939906  \n",
       "0.81   22.736584   24.453088  \n",
       "0.86   23.228545   24.963649  \n",
       "0.91   24.857606   26.672605  \n",
       "0.96   27.781894   29.667274  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing colsample_bylevel hyperparameter, i.e. the subsample ratio of the levels in each column\n",
    "# when constructing each tree\n",
    "samples = np.arange(0.01, 1.00, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for sample in samples:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bylevel = sample))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = samples,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d19843d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>1.798353</td>\n",
       "      <td>2.429155</td>\n",
       "      <td>0.602226</td>\n",
       "      <td>0.400073</td>\n",
       "      <td>4.202132</td>\n",
       "      <td>6.048055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>1.280633</td>\n",
       "      <td>2.322733</td>\n",
       "      <td>0.788227</td>\n",
       "      <td>0.448029</td>\n",
       "      <td>6.181880</td>\n",
       "      <td>7.956294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>1.004720</td>\n",
       "      <td>2.069119</td>\n",
       "      <td>0.867691</td>\n",
       "      <td>0.557191</td>\n",
       "      <td>6.791096</td>\n",
       "      <td>8.551435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>0.798174</td>\n",
       "      <td>1.814976</td>\n",
       "      <td>0.915653</td>\n",
       "      <td>0.652846</td>\n",
       "      <td>8.129776</td>\n",
       "      <td>9.874844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.643834</td>\n",
       "      <td>1.564899</td>\n",
       "      <td>0.944326</td>\n",
       "      <td>0.733532</td>\n",
       "      <td>9.412923</td>\n",
       "      <td>11.159865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.549895</td>\n",
       "      <td>1.404284</td>\n",
       "      <td>0.960203</td>\n",
       "      <td>0.784755</td>\n",
       "      <td>11.257515</td>\n",
       "      <td>12.999838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>0.492136</td>\n",
       "      <td>1.280645</td>\n",
       "      <td>0.968289</td>\n",
       "      <td>0.814784</td>\n",
       "      <td>11.694899</td>\n",
       "      <td>13.346529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>0.472485</td>\n",
       "      <td>1.264556</td>\n",
       "      <td>0.970903</td>\n",
       "      <td>0.820409</td>\n",
       "      <td>12.938853</td>\n",
       "      <td>14.670997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>0.451787</td>\n",
       "      <td>1.185316</td>\n",
       "      <td>0.973174</td>\n",
       "      <td>0.840025</td>\n",
       "      <td>14.623046</td>\n",
       "      <td>16.353415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>0.445414</td>\n",
       "      <td>1.174575</td>\n",
       "      <td>0.974025</td>\n",
       "      <td>0.842993</td>\n",
       "      <td>15.383314</td>\n",
       "      <td>17.120264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51</th>\n",
       "      <td>0.400830</td>\n",
       "      <td>1.079140</td>\n",
       "      <td>0.978969</td>\n",
       "      <td>0.865219</td>\n",
       "      <td>16.109799</td>\n",
       "      <td>17.835554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56</th>\n",
       "      <td>0.395668</td>\n",
       "      <td>1.099082</td>\n",
       "      <td>0.979558</td>\n",
       "      <td>0.859394</td>\n",
       "      <td>17.269343</td>\n",
       "      <td>19.007597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61</th>\n",
       "      <td>0.387309</td>\n",
       "      <td>1.041369</td>\n",
       "      <td>0.980409</td>\n",
       "      <td>0.874663</td>\n",
       "      <td>18.408239</td>\n",
       "      <td>20.153805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.66</th>\n",
       "      <td>0.370032</td>\n",
       "      <td>1.016352</td>\n",
       "      <td>0.982228</td>\n",
       "      <td>0.876135</td>\n",
       "      <td>19.255380</td>\n",
       "      <td>21.019861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71</th>\n",
       "      <td>0.384226</td>\n",
       "      <td>1.025160</td>\n",
       "      <td>0.980625</td>\n",
       "      <td>0.876351</td>\n",
       "      <td>20.643459</td>\n",
       "      <td>22.367352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.76</th>\n",
       "      <td>0.375324</td>\n",
       "      <td>1.012480</td>\n",
       "      <td>0.981605</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>21.491972</td>\n",
       "      <td>23.231194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.81</th>\n",
       "      <td>0.382465</td>\n",
       "      <td>1.022908</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.878211</td>\n",
       "      <td>22.415073</td>\n",
       "      <td>24.142677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.86</th>\n",
       "      <td>0.370286</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>0.982020</td>\n",
       "      <td>0.885094</td>\n",
       "      <td>23.438795</td>\n",
       "      <td>25.166226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.91</th>\n",
       "      <td>0.368111</td>\n",
       "      <td>0.970915</td>\n",
       "      <td>0.982332</td>\n",
       "      <td>0.888198</td>\n",
       "      <td>25.258927</td>\n",
       "      <td>27.122616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.353062</td>\n",
       "      <td>0.947312</td>\n",
       "      <td>0.983645</td>\n",
       "      <td>0.892316</td>\n",
       "      <td>26.968165</td>\n",
       "      <td>28.731891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.01           1.798353             2.429155      0.602226        0.400073   \n",
       "0.06           1.280633             2.322733      0.788227        0.448029   \n",
       "0.11           1.004720             2.069119      0.867691        0.557191   \n",
       "0.16           0.798174             1.814976      0.915653        0.652846   \n",
       "0.21           0.643834             1.564899      0.944326        0.733532   \n",
       "0.26           0.549895             1.404284      0.960203        0.784755   \n",
       "0.31           0.492136             1.280645      0.968289        0.814784   \n",
       "0.36           0.472485             1.264556      0.970903        0.820409   \n",
       "0.41           0.451787             1.185316      0.973174        0.840025   \n",
       "0.46           0.445414             1.174575      0.974025        0.842993   \n",
       "0.51           0.400830             1.079140      0.978969        0.865219   \n",
       "0.56           0.395668             1.099082      0.979558        0.859394   \n",
       "0.61           0.387309             1.041369      0.980409        0.874663   \n",
       "0.66           0.370032             1.016352      0.982228        0.876135   \n",
       "0.71           0.384226             1.025160      0.980625        0.876351   \n",
       "0.76           0.375324             1.012480      0.981605        0.879747   \n",
       "0.81           0.382465             1.022908      0.980805        0.878211   \n",
       "0.86           0.370286             0.983759      0.982020        0.885094   \n",
       "0.91           0.368111             0.970915      0.982332        0.888198   \n",
       "0.96           0.353062             0.947312      0.983645        0.892316   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.01    4.202132    6.048055  \n",
       "0.06    6.181880    7.956294  \n",
       "0.11    6.791096    8.551435  \n",
       "0.16    8.129776    9.874844  \n",
       "0.21    9.412923   11.159865  \n",
       "0.26   11.257515   12.999838  \n",
       "0.31   11.694899   13.346529  \n",
       "0.36   12.938853   14.670997  \n",
       "0.41   14.623046   16.353415  \n",
       "0.46   15.383314   17.120264  \n",
       "0.51   16.109799   17.835554  \n",
       "0.56   17.269343   19.007597  \n",
       "0.61   18.408239   20.153805  \n",
       "0.66   19.255380   21.019861  \n",
       "0.71   20.643459   22.367352  \n",
       "0.76   21.491972   23.231194  \n",
       "0.81   22.415073   24.142677  \n",
       "0.86   23.438795   25.166226  \n",
       "0.91   25.258927   27.122616  \n",
       "0.96   26.968165   28.731891  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing colsample_bynode hyperparameter, i.e. the subsample ratio for each column when splitting\n",
    "# a branch to make two new branches\n",
    "samples = np.arange(0.01, 1.00, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for sample in samples:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bynode = sample))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = samples,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8e4cd2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>4.413629</td>\n",
       "      <td>5.056001</td>\n",
       "      <td>-1.516213</td>\n",
       "      <td>-1.517837</td>\n",
       "      <td>10.295430</td>\n",
       "      <td>12.254829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>1.415935</td>\n",
       "      <td>1.981269</td>\n",
       "      <td>0.738513</td>\n",
       "      <td>0.589954</td>\n",
       "      <td>11.379794</td>\n",
       "      <td>13.175313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>0.989876</td>\n",
       "      <td>1.509975</td>\n",
       "      <td>0.873296</td>\n",
       "      <td>0.755453</td>\n",
       "      <td>13.080247</td>\n",
       "      <td>15.124846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>0.790955</td>\n",
       "      <td>1.260727</td>\n",
       "      <td>0.918798</td>\n",
       "      <td>0.824278</td>\n",
       "      <td>14.655134</td>\n",
       "      <td>16.568756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.724512</td>\n",
       "      <td>1.234590</td>\n",
       "      <td>0.932528</td>\n",
       "      <td>0.835016</td>\n",
       "      <td>15.820757</td>\n",
       "      <td>17.699851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.26</th>\n",
       "      <td>0.642981</td>\n",
       "      <td>1.174714</td>\n",
       "      <td>0.946760</td>\n",
       "      <td>0.847054</td>\n",
       "      <td>16.901674</td>\n",
       "      <td>18.648818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <td>0.581012</td>\n",
       "      <td>1.103267</td>\n",
       "      <td>0.956601</td>\n",
       "      <td>0.864621</td>\n",
       "      <td>18.181415</td>\n",
       "      <td>20.054232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.36</th>\n",
       "      <td>0.525506</td>\n",
       "      <td>1.053003</td>\n",
       "      <td>0.964514</td>\n",
       "      <td>0.873908</td>\n",
       "      <td>19.954895</td>\n",
       "      <td>21.827754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.41</th>\n",
       "      <td>0.488436</td>\n",
       "      <td>1.004780</td>\n",
       "      <td>0.969156</td>\n",
       "      <td>0.883887</td>\n",
       "      <td>20.794802</td>\n",
       "      <td>22.701168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.46</th>\n",
       "      <td>0.477752</td>\n",
       "      <td>1.014751</td>\n",
       "      <td>0.970545</td>\n",
       "      <td>0.881267</td>\n",
       "      <td>22.304943</td>\n",
       "      <td>24.073319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.51</th>\n",
       "      <td>0.457228</td>\n",
       "      <td>0.991545</td>\n",
       "      <td>0.973208</td>\n",
       "      <td>0.887387</td>\n",
       "      <td>22.926894</td>\n",
       "      <td>24.830544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56</th>\n",
       "      <td>0.426950</td>\n",
       "      <td>0.972608</td>\n",
       "      <td>0.976375</td>\n",
       "      <td>0.888176</td>\n",
       "      <td>25.101878</td>\n",
       "      <td>27.013715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.61</th>\n",
       "      <td>0.414406</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.977711</td>\n",
       "      <td>0.889208</td>\n",
       "      <td>24.348138</td>\n",
       "      <td>26.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.66</th>\n",
       "      <td>0.403685</td>\n",
       "      <td>0.972238</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>0.887924</td>\n",
       "      <td>25.016746</td>\n",
       "      <td>26.912483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.71</th>\n",
       "      <td>0.400885</td>\n",
       "      <td>0.959621</td>\n",
       "      <td>0.979121</td>\n",
       "      <td>0.890826</td>\n",
       "      <td>26.238134</td>\n",
       "      <td>28.120122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.76</th>\n",
       "      <td>0.375023</td>\n",
       "      <td>0.932198</td>\n",
       "      <td>0.981686</td>\n",
       "      <td>0.896503</td>\n",
       "      <td>25.857518</td>\n",
       "      <td>27.730936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.81</th>\n",
       "      <td>0.364077</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.982770</td>\n",
       "      <td>0.895641</td>\n",
       "      <td>26.344613</td>\n",
       "      <td>28.226944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.86</th>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.930853</td>\n",
       "      <td>0.982888</td>\n",
       "      <td>0.895668</td>\n",
       "      <td>26.609042</td>\n",
       "      <td>28.486780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.91</th>\n",
       "      <td>0.348205</td>\n",
       "      <td>0.921843</td>\n",
       "      <td>0.984217</td>\n",
       "      <td>0.896190</td>\n",
       "      <td>26.792883</td>\n",
       "      <td>28.672751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.353073</td>\n",
       "      <td>0.918369</td>\n",
       "      <td>0.983680</td>\n",
       "      <td>0.898798</td>\n",
       "      <td>27.036810</td>\n",
       "      <td>28.924885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0.01           4.413629             5.056001     -1.516213       -1.517837   \n",
       "0.06           1.415935             1.981269      0.738513        0.589954   \n",
       "0.11           0.989876             1.509975      0.873296        0.755453   \n",
       "0.16           0.790955             1.260727      0.918798        0.824278   \n",
       "0.21           0.724512             1.234590      0.932528        0.835016   \n",
       "0.26           0.642981             1.174714      0.946760        0.847054   \n",
       "0.31           0.581012             1.103267      0.956601        0.864621   \n",
       "0.36           0.525506             1.053003      0.964514        0.873908   \n",
       "0.41           0.488436             1.004780      0.969156        0.883887   \n",
       "0.46           0.477752             1.014751      0.970545        0.881267   \n",
       "0.51           0.457228             0.991545      0.973208        0.887387   \n",
       "0.56           0.426950             0.972608      0.976375        0.888176   \n",
       "0.61           0.414406             0.967682      0.977711        0.889208   \n",
       "0.66           0.403685             0.972238      0.978731        0.887924   \n",
       "0.71           0.400885             0.959621      0.979121        0.890826   \n",
       "0.76           0.375023             0.932198      0.981686        0.896503   \n",
       "0.81           0.364077             0.934911      0.982770        0.895641   \n",
       "0.86           0.361800             0.930853      0.982888        0.895668   \n",
       "0.91           0.348205             0.921843      0.984217        0.896190   \n",
       "0.96           0.353073             0.918369      0.983680        0.898798   \n",
       "\n",
       "      Train Time  Score Time  \n",
       "0.01   10.295430   12.254829  \n",
       "0.06   11.379794   13.175313  \n",
       "0.11   13.080247   15.124846  \n",
       "0.16   14.655134   16.568756  \n",
       "0.21   15.820757   17.699851  \n",
       "0.26   16.901674   18.648818  \n",
       "0.31   18.181415   20.054232  \n",
       "0.36   19.954895   21.827754  \n",
       "0.41   20.794802   22.701168  \n",
       "0.46   22.304943   24.073319  \n",
       "0.51   22.926894   24.830544  \n",
       "0.56   25.101878   27.013715  \n",
       "0.61   24.348138   26.247300  \n",
       "0.66   25.016746   26.912483  \n",
       "0.71   26.238134   28.120122  \n",
       "0.76   25.857518   27.730936  \n",
       "0.81   26.344613   28.226944  \n",
       "0.86   26.609042   28.486780  \n",
       "0.91   26.792883   28.672751  \n",
       "0.96   27.036810   28.924885  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing subsamples\n",
    "subsamples = np.arange(0.01, 1.00, 0.05)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for sample in subsamples:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        subsample = sample))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = subsamples,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])\n",
    "# we find that the best subsample value is the default value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1ae53607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:11:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, max_depth } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gbtree</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>27.401877</td>\n",
       "      <td>29.275304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gblinear</th>\n",
       "      <td>1.970157</td>\n",
       "      <td>2.330319</td>\n",
       "      <td>0.529298</td>\n",
       "      <td>0.430439</td>\n",
       "      <td>2.876022</td>\n",
       "      <td>4.479491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dart</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>36.064976</td>\n",
       "      <td>38.148283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Training Accuracy  Validation Accuracy  Training R^2  \\\n",
       "gbtree             0.344965             0.899407      0.984345   \n",
       "gblinear           1.970157             2.330319      0.529298   \n",
       "dart               0.344965             0.899407      0.984345   \n",
       "\n",
       "          Validation R^2  Train Time  Score Time  \n",
       "gbtree          0.902267   27.401877   29.275304  \n",
       "gblinear        0.430439    2.876022    4.479491  \n",
       "dart            0.902267   36.064976   38.148283  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning if a different boosting method would provide greater accuracy\n",
    "boosters = ['gbtree', 'gblinear', 'dart']\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for boost in boosters:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        booster = boost))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = boosters,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "409da94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>27.506483</td>\n",
       "      <td>29.377646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.391629</td>\n",
       "      <td>0.947911</td>\n",
       "      <td>0.979508</td>\n",
       "      <td>0.891173</td>\n",
       "      <td>26.916559</td>\n",
       "      <td>28.807611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.439008</td>\n",
       "      <td>0.958825</td>\n",
       "      <td>0.974060</td>\n",
       "      <td>0.889524</td>\n",
       "      <td>27.413523</td>\n",
       "      <td>29.276994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.454317</td>\n",
       "      <td>0.952485</td>\n",
       "      <td>0.971973</td>\n",
       "      <td>0.890390</td>\n",
       "      <td>27.345217</td>\n",
       "      <td>29.263388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.472256</td>\n",
       "      <td>0.954751</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.888598</td>\n",
       "      <td>26.879322</td>\n",
       "      <td>28.780426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.492763</td>\n",
       "      <td>0.982929</td>\n",
       "      <td>0.966583</td>\n",
       "      <td>0.880468</td>\n",
       "      <td>26.758655</td>\n",
       "      <td>28.575058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.508679</td>\n",
       "      <td>0.989497</td>\n",
       "      <td>0.964250</td>\n",
       "      <td>0.879750</td>\n",
       "      <td>26.708717</td>\n",
       "      <td>28.586421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.523687</td>\n",
       "      <td>1.003239</td>\n",
       "      <td>0.962044</td>\n",
       "      <td>0.877804</td>\n",
       "      <td>26.462917</td>\n",
       "      <td>28.352404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.522452</td>\n",
       "      <td>0.989624</td>\n",
       "      <td>0.961625</td>\n",
       "      <td>0.878296</td>\n",
       "      <td>26.629544</td>\n",
       "      <td>28.411969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.545746</td>\n",
       "      <td>1.006524</td>\n",
       "      <td>0.957863</td>\n",
       "      <td>0.874862</td>\n",
       "      <td>26.396922</td>\n",
       "      <td>28.277545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.985826</td>\n",
       "      <td>0.959516</td>\n",
       "      <td>0.881749</td>\n",
       "      <td>26.239194</td>\n",
       "      <td>28.098806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.556834</td>\n",
       "      <td>1.019015</td>\n",
       "      <td>0.956249</td>\n",
       "      <td>0.872301</td>\n",
       "      <td>26.582281</td>\n",
       "      <td>28.476501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.563799</td>\n",
       "      <td>1.022314</td>\n",
       "      <td>0.955009</td>\n",
       "      <td>0.871295</td>\n",
       "      <td>26.214096</td>\n",
       "      <td>28.047742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.585925</td>\n",
       "      <td>1.046320</td>\n",
       "      <td>0.951506</td>\n",
       "      <td>0.866360</td>\n",
       "      <td>25.968487</td>\n",
       "      <td>27.815338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.594736</td>\n",
       "      <td>1.058536</td>\n",
       "      <td>0.950303</td>\n",
       "      <td>0.862978</td>\n",
       "      <td>26.286542</td>\n",
       "      <td>28.121358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.594781</td>\n",
       "      <td>1.058039</td>\n",
       "      <td>0.949366</td>\n",
       "      <td>0.862594</td>\n",
       "      <td>26.155226</td>\n",
       "      <td>28.018690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.604440</td>\n",
       "      <td>1.058421</td>\n",
       "      <td>0.947926</td>\n",
       "      <td>0.862356</td>\n",
       "      <td>26.130138</td>\n",
       "      <td>28.018665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.613286</td>\n",
       "      <td>1.070827</td>\n",
       "      <td>0.946281</td>\n",
       "      <td>0.858388</td>\n",
       "      <td>26.556529</td>\n",
       "      <td>28.464356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.611360</td>\n",
       "      <td>1.070732</td>\n",
       "      <td>0.946448</td>\n",
       "      <td>0.859236</td>\n",
       "      <td>25.852909</td>\n",
       "      <td>27.729950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.621784</td>\n",
       "      <td>1.100838</td>\n",
       "      <td>0.943656</td>\n",
       "      <td>0.850288</td>\n",
       "      <td>26.132979</td>\n",
       "      <td>27.927537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0            0.344965             0.899407      0.984345        0.902267   \n",
       "5            0.391629             0.947911      0.979508        0.891173   \n",
       "10           0.439008             0.958825      0.974060        0.889524   \n",
       "15           0.454317             0.952485      0.971973        0.890390   \n",
       "20           0.472256             0.954751      0.969423        0.888598   \n",
       "25           0.492763             0.982929      0.966583        0.880468   \n",
       "30           0.508679             0.989497      0.964250        0.879750   \n",
       "35           0.523687             1.003239      0.962044        0.877804   \n",
       "40           0.522452             0.989624      0.961625        0.878296   \n",
       "45           0.545746             1.006524      0.957863        0.874862   \n",
       "50           0.538432             0.985826      0.959516        0.881749   \n",
       "55           0.556834             1.019015      0.956249        0.872301   \n",
       "60           0.563799             1.022314      0.955009        0.871295   \n",
       "65           0.585925             1.046320      0.951506        0.866360   \n",
       "70           0.594736             1.058536      0.950303        0.862978   \n",
       "75           0.594781             1.058039      0.949366        0.862594   \n",
       "80           0.604440             1.058421      0.947926        0.862356   \n",
       "85           0.613286             1.070827      0.946281        0.858388   \n",
       "90           0.611360             1.070732      0.946448        0.859236   \n",
       "95           0.621784             1.100838      0.943656        0.850288   \n",
       "\n",
       "    Train Time  Score Time  \n",
       "0    27.506483   29.377646  \n",
       "5    26.916559   28.807611  \n",
       "10   27.413523   29.276994  \n",
       "15   27.345217   29.263388  \n",
       "20   26.879322   28.780426  \n",
       "25   26.758655   28.575058  \n",
       "30   26.708717   28.586421  \n",
       "35   26.462917   28.352404  \n",
       "40   26.629544   28.411969  \n",
       "45   26.396922   28.277545  \n",
       "50   26.239194   28.098806  \n",
       "55   26.582281   28.476501  \n",
       "60   26.214096   28.047742  \n",
       "65   25.968487   27.815338  \n",
       "70   26.286542   28.121358  \n",
       "75   26.155226   28.018690  \n",
       "80   26.130138   28.018665  \n",
       "85   26.556529   28.464356  \n",
       "90   25.852909   27.729950  \n",
       "95   26.132979   27.927537  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing alphas hyperparameter\n",
    "alphas = range(0, 100, 5)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_alpha = alpha))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = alphas,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "34b7ce8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.336003</td>\n",
       "      <td>0.823631</td>\n",
       "      <td>0.984873</td>\n",
       "      <td>0.906840</td>\n",
       "      <td>25.307738</td>\n",
       "      <td>25.522128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.356787</td>\n",
       "      <td>0.829935</td>\n",
       "      <td>0.982614</td>\n",
       "      <td>0.905933</td>\n",
       "      <td>24.938883</td>\n",
       "      <td>25.153915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358501</td>\n",
       "      <td>0.811367</td>\n",
       "      <td>0.982295</td>\n",
       "      <td>0.909464</td>\n",
       "      <td>26.223801</td>\n",
       "      <td>26.459113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.372988</td>\n",
       "      <td>0.820176</td>\n",
       "      <td>0.980786</td>\n",
       "      <td>0.906642</td>\n",
       "      <td>25.276711</td>\n",
       "      <td>25.498571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.389040</td>\n",
       "      <td>0.833858</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.904995</td>\n",
       "      <td>24.922245</td>\n",
       "      <td>25.157767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0           0.336003             0.823631      0.984873        0.906840   \n",
       "1           0.356787             0.829935      0.982614        0.905933   \n",
       "2           0.358501             0.811367      0.982295        0.909464   \n",
       "3           0.372988             0.820176      0.980786        0.906642   \n",
       "4           0.389040             0.833858      0.978819        0.904995   \n",
       "\n",
       "   Train Time  Score Time  \n",
       "0   25.307738   25.522128  \n",
       "1   24.938883   25.153915  \n",
       "2   26.223801   26.459113  \n",
       "3   25.276711   25.498571  \n",
       "4   24.922245   25.157767  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finish optimizing alphas hyperparameter\n",
    "alphas = range(0, 5)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_alpha = alpha))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_tune_train, model.predict(X_tune_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_tune_val, model.predict(X_tune_val)))\n",
    "    train_r2_boost.append(model.score(X_tune_train, y_tune_train))\n",
    "    val_r2_boost.append(model.score(X_tune_val, y_tune_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = alphas,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f501695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>26.907414</td>\n",
       "      <td>28.600229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.344977</td>\n",
       "      <td>0.912185</td>\n",
       "      <td>0.984208</td>\n",
       "      <td>0.898698</td>\n",
       "      <td>1987.020911</td>\n",
       "      <td>1989.366159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.339320</td>\n",
       "      <td>0.884291</td>\n",
       "      <td>0.984564</td>\n",
       "      <td>0.904754</td>\n",
       "      <td>26.078574</td>\n",
       "      <td>27.823195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.362562</td>\n",
       "      <td>0.926158</td>\n",
       "      <td>0.982281</td>\n",
       "      <td>0.895772</td>\n",
       "      <td>26.777786</td>\n",
       "      <td>28.631919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.363661</td>\n",
       "      <td>0.928849</td>\n",
       "      <td>0.982134</td>\n",
       "      <td>0.894289</td>\n",
       "      <td>27.687171</td>\n",
       "      <td>29.601385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "1           0.344965             0.899407      0.984345        0.902267   \n",
       "2           0.344977             0.912185      0.984208        0.898698   \n",
       "3           0.339320             0.884291      0.984564        0.904754   \n",
       "4           0.362562             0.926158      0.982281        0.895772   \n",
       "5           0.363661             0.928849      0.982134        0.894289   \n",
       "\n",
       "    Train Time   Score Time  \n",
       "1    26.907414    28.600229  \n",
       "2  1987.020911  1989.366159  \n",
       "3    26.078574    27.823195  \n",
       "4    26.777786    28.631919  \n",
       "5    27.687171    29.601385  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing lambda hyperparameter\n",
    "lambdas = range(1, 6)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for l in lambdas:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_lambda = l))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = lambdas,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bc30041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>26.758340</td>\n",
       "      <td>28.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.351917</td>\n",
       "      <td>0.914928</td>\n",
       "      <td>0.983926</td>\n",
       "      <td>0.899822</td>\n",
       "      <td>25.123686</td>\n",
       "      <td>26.877084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373214</td>\n",
       "      <td>0.905837</td>\n",
       "      <td>0.982175</td>\n",
       "      <td>0.901593</td>\n",
       "      <td>24.862436</td>\n",
       "      <td>26.648125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.398663</td>\n",
       "      <td>0.913498</td>\n",
       "      <td>0.979627</td>\n",
       "      <td>0.899555</td>\n",
       "      <td>24.928960</td>\n",
       "      <td>26.685407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.412089</td>\n",
       "      <td>0.895746</td>\n",
       "      <td>0.978274</td>\n",
       "      <td>0.902462</td>\n",
       "      <td>24.941093</td>\n",
       "      <td>26.703442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.434087</td>\n",
       "      <td>0.923174</td>\n",
       "      <td>0.975846</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>25.998106</td>\n",
       "      <td>27.843100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "0           0.344965             0.899407      0.984345        0.902267   \n",
       "1           0.351917             0.914928      0.983926        0.899822   \n",
       "2           0.373214             0.905837      0.982175        0.901593   \n",
       "3           0.398663             0.913498      0.979627        0.899555   \n",
       "4           0.412089             0.895746      0.978274        0.902462   \n",
       "5           0.434087             0.923174      0.975846        0.898701   \n",
       "\n",
       "   Train Time  Score Time  \n",
       "0   26.758340   28.515100  \n",
       "1   25.123686   26.877084  \n",
       "2   24.862436   26.648125  \n",
       "3   24.928960   26.685407  \n",
       "4   24.941093   26.703442  \n",
       "5   25.998106   27.843100  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing gamma hyperparameter\n",
    "gammas = range(0, 6)\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for g in gammas:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = 'mean'),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        gamma = g))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = gammas,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5049bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training R^2</th>\n",
       "      <th>Validation R^2</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Score Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.344965</td>\n",
       "      <td>0.899407</td>\n",
       "      <td>0.984345</td>\n",
       "      <td>0.902267</td>\n",
       "      <td>26.481550</td>\n",
       "      <td>28.215710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.334195</td>\n",
       "      <td>0.900944</td>\n",
       "      <td>0.985359</td>\n",
       "      <td>0.901148</td>\n",
       "      <td>26.680176</td>\n",
       "      <td>28.502702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Training Accuracy  Validation Accuracy  Training R^2  Validation R^2  \\\n",
       "mean             0.344965             0.899407      0.984345        0.902267   \n",
       "median           0.334195             0.900944      0.985359        0.901148   \n",
       "\n",
       "        Train Time  Score Time  \n",
       "mean     26.481550   28.215710  \n",
       "median   26.680176   28.502702  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should we choose median instead of mean for SimpleImputer?\n",
    "strats = ['mean', 'median']\n",
    "train_acc_boost = []\n",
    "val_acc_boost = []\n",
    "train_r2_boost = []\n",
    "val_r2_boost = []\n",
    "train_times = []\n",
    "score_times = []\n",
    "\n",
    "for strat in strats:\n",
    "    start = time.time()\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                         StandardScaler(),\n",
    "                         SimpleImputer(strategy = strat),\n",
    "                         XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_alpha = 0,\n",
    "                                        reg_lambda = 1,\n",
    "                                        gamma = 0))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_times.append(time.time() - start)\n",
    "    train_acc_boost.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_boost.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    train_r2_boost.append(model.score(X_train, y_train))\n",
    "    val_r2_boost.append(model.score(X_val, y_val))\n",
    "    score_times.append(time.time() - start)\n",
    "\n",
    "pd.DataFrame(list(zip(train_acc_boost, val_acc_boost, train_r2_boost, val_r2_boost, train_times, score_times)), \n",
    "             index = strats,\n",
    "             columns = ['Training Accuracy', 'Validation Accuracy', 'Training R^2', 'Validation R^2', \n",
    "                        'Train Time', 'Score Time'])\n",
    "# mean works best, as expected - but median works surprisingly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "33a03bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to build our final model with all our hyperparameters\n",
    "model_boost = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                           StandardScaler(),\n",
    "                           SimpleImputer(strategy='mean'),\n",
    "                           XGBRegressor(random_state = 42,\n",
    "                                        n_jobs = -1,\n",
    "                                        max_depth = 9,\n",
    "                                        n_estimators = 100,\n",
    "                                        learning_rate = 0.29,\n",
    "                                        colsample_bytree = 1,\n",
    "                                        reg_alpha = 0,\n",
    "                                        reg_lambda = 1,\n",
    "                                        gamma = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14a52c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 2.35 s, total: 2min 53s\n",
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['home_ownership', 'verification_status',\n",
       "                                     'purpose'],\n",
       "                               use_cat_names=True)),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='', learning_rate=0.29,\n",
       "                              max_delta_step=0, max_depth=9, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=100, n_jobs=-1, num_parallel_tree=1,\n",
       "                              random_state=42, reg_alpha=0, reg_lambda=1,\n",
       "                              scale_pos_weight=1, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49ffb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.34496464560571966\n",
      "Validation MAE: 0.8994065691179319\n",
      "Validation R2: 0.9022665324154702\n",
      "CPU times: user 3.83 s, sys: 335 ms, total: 4.17 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_boost.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_boost.predict(X_val)))\n",
    "print('Validation R2:', model_boost.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "860dd6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9123584023146762\n"
     ]
    }
   ],
   "source": [
    "# check cross validation score\n",
    "cross_vals = cross_val_score(model_boost, X, y)\n",
    "print(np.mean(cross_vals))\n",
    "# it's actually higher than our previous R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "05acad5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model_boost.named_steps['xgbregressor'].feature_importances_\n",
    "features = model_boost.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp_boost = pd.Series(importances, index = features,\n",
    "                          name = 'boost').abs().sort_values(ascending=False)\n",
    "feat_imp_boost.head(10).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b26450",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d1481090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.821935</td>\n",
       "      <td>2.150289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.822044</td>\n",
       "      <td>2.149810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.822115</td>\n",
       "      <td>2.149895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.822186</td>\n",
       "      <td>2.149980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.822258</td>\n",
       "      <td>2.150067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.822330</td>\n",
       "      <td>2.150154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training Accuracy  Validation Accuracy\n",
       "0           1.821935             2.150289\n",
       "1           1.822044             2.149810\n",
       "2           1.822115             2.149895\n",
       "3           1.822186             2.149980\n",
       "4           1.822258             2.150067\n",
       "5           1.822330             2.150154"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.arange(0, 6)\n",
    "train_acc_ridge = []\n",
    "val_acc_ridge = []\n",
    "for a in alphas:\n",
    "    model = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                            StandardScaler(),\n",
    "                            SimpleImputer(strategy = 'mean'),\n",
    "                            Ridge(alpha = a))\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc_ridge.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "    val_acc_ridge.append(mean_absolute_error(y_val, model.predict(X_val)))\n",
    "    \n",
    "pd.DataFrame(list(zip(train_acc_ridge, val_acc_ridge)), index = alphas, \n",
    "             columns = ['Training Accuracy', 'Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c09cc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                            StandardScaler(),\n",
    "                            SimpleImputer(strategy = 'mean'),\n",
    "                            Ridge(alpha = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7a88bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 1.8220438129808365\n",
      "Validation MAE: 2.1498096703868845\n",
      "Validation R2: 0.5212556280653425\n"
     ]
    }
   ],
   "source": [
    "model_ridge.fit(X_train, y_train)\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_ridge.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_ridge.predict(X_val)))\n",
    "print('Validation R2:', model_ridge.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7abd543e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model_ridge.named_steps['ridge'].coef_\n",
    "features = model_ridge.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp_ridge = pd.Series(importances, index = features,\n",
    "                          name = 'ridge').abs().sort_values(ascending=False)\n",
    "feat_imp_ridge.head(10).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14ec43",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c05fd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = make_pipeline(OneHotEncoder(use_cat_names = True),\n",
    "                            StandardScaler(),\n",
    "                            SimpleImputer(strategy = 'mean'),\n",
    "                            LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0a5b46db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 1.821964672670216\n",
      "Validation MAE: 2.1497278134142492\n",
      "Validation R2: 0.5212789361051063\n"
     ]
    }
   ],
   "source": [
    "model_linear.fit(X_train, y_train)\n",
    "print('Training MAE:', mean_absolute_error(y_train, model_linear.predict(X_train)))\n",
    "print('Validation MAE:', mean_absolute_error(y_val, model_linear.predict(X_val)))\n",
    "print('Validation R2:', model_linear.score(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2168f8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = model_linear.named_steps['linearregression'].coef_\n",
    "features = model_linear.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp_linear = pd.Series(importances, index = features,\n",
    "                           name = 'linear').abs().sort_values(ascending=False)\n",
    "feat_imp_linear.head(10).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63e865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
